#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å–®ä¸€è‚¡ç¥¨ 10-K Filing Items Parser
åŸºæ–¼ parse_10k_items.pyï¼Œå°ˆé–€è™•ç†å–®ä¸€è‚¡ç¥¨çš„è²¡å ±è§£æ
å„ªåŒ–ç‰ˆæœ¬ï¼šå¢åŠ æ€§èƒ½ç›£æ§å’Œå¿«é€Ÿè™•ç†æ¨¡å¼
"""

import os
import re
import sys
import mysql.connector
from mysql.connector import Error
from datetime import datetime
from pathlib import Path
import hashlib
import time

class SingleStockTenKParser:
    def __init__(self, ticker, db_config=None):
        self.ticker = ticker.upper()
        self.start_time = time.time()
        
        # è³‡æ–™åº«é…ç½®
        self.db_config = db_config or {
            'host': '43.207.210.147',
            'database': 'finbot_db',
            'user': 'myuser',
            'password': '123456789',
            'charset': 'utf8mb4'
        }
        
        self.db_connection = None
        
        # ç·¨è­¯æ­£å‰‡è¡¨é”å¼ä»¥æé«˜æ€§èƒ½
        self.compiled_patterns = {}
        raw_patterns = {
            'item_1': r'Item\s+1\.\s*(?:Business|BUSINESS|$)',
            'item_1a': r'Item\s+1A\.\s*(?:Risk Factors|RISK FACTORS|$)',
            'item_1b': r'Item\s+1B\.\s*(?:Unresolved Staff Comments|UNRESOLVED STAFF COMMENTS|$)',
            'item_2': r'Item\s+2\.\s*(?:Properties|PROPERTIES|$)',
            'item_3': r'Item\s+3\.\s*(?:Legal Proceedings|LEGAL PROCEEDINGS|$)',
            'item_4': r'Item\s+4\.\s*(?:Mine Safety|MINE SAFETY|$)',
            'item_5': r'Item\s+5\.\s*(?:Market for Registrant|MARKET FOR REGISTRANT|$)',
            'item_6': r'Item\s+6\.\s*(?:Selected Financial Data|SELECTED FINANCIAL DATA|$)',
            'item_7': r'Item\s+7\.\s*(?:Management|MANAGEMENT|$)',
            'item_7a': r'Item\s+7A\.\s*(?:Quantitative and Qualitative|QUANTITATIVE AND QUALITATIVE|$)',
            'item_8': r'Item\s+8\.\s*(?:Financial Statements|FINANCIAL STATEMENTS|$)',
            'item_9': r'Item\s+9\.\s*(?:Changes in and Disagreements|CHANGES IN AND DISAGREEMENTS|$)',
            'item_9a': r'Item\s+9A\.\s*(?:Controls and Procedures|CONTROLS AND PROCEDURES|$)',
            'item_9b': r'Item\s+9B\.\s*(?:Other Information|OTHER INFORMATION|$)',
            'item_10': r'Item\s+10\.\s*(?:Directors|DIRECTORS|$)',
            'item_11': r'Item\s+11\.\s*(?:Executive Compensation|EXECUTIVE COMPENSATION|$)',
            'item_12': r'Item\s+12\.\s*(?:Security Ownership|SECURITY OWNERSHIP|$)',
            'item_13': r'Item\s+13\.\s*(?:Certain Relationships|CERTAIN RELATIONSHIPS|$)',
            'item_14': r'Item\s+14\.\s*(?:Principal Accountant|PRINCIPAL ACCOUNTANT|$)',
            'item_15': r'Item\s+15\.\s*(?:Exhibits|EXHIBITS|$)',
            'item_16': r'Item\s+16\.\s*(?:Form 10-K Summary|FORM 10-K SUMMARY|$)',
            'appendix': r'(?:INDEX\s+TO\s+FINANCIAL\s+STATEMENTS|APPENDIX|CONSOLIDATED\s+FINANCIAL\s+STATEMENTS)'
        }
        
        # é ç·¨è­¯æ­£å‰‡è¡¨é”å¼
        print("ğŸš€ é ç·¨è­¯æ­£å‰‡è¡¨é”å¼ä»¥æé«˜æ€§èƒ½...")
        for key, pattern in raw_patterns.items():
            self.compiled_patterns[key] = re.compile(pattern, re.IGNORECASE | re.MULTILINE)
        
        # å¸¸ç”¨çš„æ¸…ç†æ¨¡å¼ä¹Ÿé ç·¨è­¯
        self.html_style_pattern = re.compile(r'<style[^>]*>.*?</style>', re.DOTALL | re.IGNORECASE)
        self.html_script_pattern = re.compile(r'<script[^>]*>.*?</script>', re.DOTALL | re.IGNORECASE)
        self.html_tag_pattern = re.compile(r'<[^>]+>')
        self.html_entity_pattern = re.compile(r'&#\d+;')
        self.whitespace_pattern = re.compile(r'\s+')
        
        print(f"âœ… åˆå§‹åŒ–å®Œæˆ ({self.ticker})")

    def log_performance(self, stage, duration=None):
        """è¨˜éŒ„æ€§èƒ½è³‡è¨Š"""
        current_time = time.time()
        total_elapsed = current_time - self.start_time
        if duration is None:
            print(f"â±ï¸ [{stage}] ç¸½è€—æ™‚: {total_elapsed:.2f}ç§’")
        else:
            print(f"â±ï¸ [{stage}] æœ¬éšæ®µ: {duration:.2f}ç§’, ç¸½è€—æ™‚: {total_elapsed:.2f}ç§’")

    def connect_database(self):
        """é€£æ¥è³‡æ–™åº«"""
        try:
            self.db_connection = mysql.connector.connect(**self.db_config)
            if self.db_connection.is_connected():
                print("âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
                return True
        except Error as e:
            print(f"âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
            return False

    def disconnect_database(self):
        """æ–·é–‹è³‡æ–™åº«é€£æ¥"""
        if self.db_connection and self.db_connection.is_connected():
            self.db_connection.close()
            print("âœ… è³‡æ–™åº«é€£æ¥å·²é—œé–‰")

    def extract_filing_metadata(self, content):
        """æå–10-Kå ±å‘Šçš„åŸºæœ¬è³‡è¨Š"""
        metadata = {}
        
        # æå–æ–‡ä»¶ç·¨è™Ÿ
        doc_number_pattern = r'DOCUMENT\s+(\d{10}-\d{2}-\d{6})'
        doc_match = re.search(doc_number_pattern, content)
        metadata['document_number'] = doc_match.group(1) if doc_match else None
        
        # ä½¿ç”¨è‚¡ç¥¨ä»£è™Ÿä½œç‚ºå…¬å¸åç¨±
        metadata['company_name'] = self.ticker
        
        # æå–CIK
        cik_pattern = r'CENTRAL INDEX KEY:\s+(\d+)'
        cik_match = re.search(cik_pattern, content)
        metadata['cik'] = cik_match.group(1) if cik_match else None
        
        # æå–å ±å‘Šæ—¥æœŸ
        date_pattern = r'CONFORMED PERIOD OF REPORT:\s+(\d{8})'
        date_match = re.search(date_pattern, content)
        if date_match:
            date_str = date_match.group(1)
            metadata['report_date'] = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        else:
            metadata['report_date'] = None
        
        # æå–æäº¤æ—¥æœŸ
        filed_pattern = r'FILED AS OF DATE:\s+(\d{8})'
        filed_match = re.search(filed_pattern, content)
        if filed_match:
            filed_str = filed_match.group(1)
            metadata['filed_date'] = f"{filed_str[:4]}-{filed_str[4:6]}-{filed_str[6:8]}"
        else:
            metadata['filed_date'] = None
        
        return metadata

    def find_content_start_position(self, content):
        """æ‰¾åˆ°å¯¦éš›å…§å®¹é–‹å§‹ä½ç½®ï¼Œè·³éTABLE OF CONTENTS"""
        
        # ç›´æ¥å°‹æ‰¾æ‰€æœ‰ Item 1 çš„ä½ç½®ï¼Œç„¶å¾Œé¸æ“‡çœŸæ­£åŒ…å«å…§å®¹çš„é‚£ä¸€å€‹
        item1_matches = list(re.finditer(r'Item\s+1\.', content, re.IGNORECASE | re.MULTILINE))
        
        print(f"   ğŸ“ æ‰¾åˆ° {len(item1_matches)} å€‹ Item 1 ä½ç½®")
        
        for i, match in enumerate(item1_matches):
            match_pos = match.start()
            # æª¢æŸ¥é€™å€‹ä½ç½®å¾Œé¢æ›´å¤§ç¯„åœçš„å…§å®¹ï¼ˆæé«˜åˆ°3000å­—ç¬¦ï¼‰
            sample_content = content[match_pos:match_pos + 3000]
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«çœŸå¯¦çš„æ¥­å‹™å…§å®¹
            if self.looks_like_real_content(sample_content):
                print(f"   ğŸ¯ é¸æ“‡ä½ç½® {i+1}: {match_pos} (åŒ…å«çœŸå¯¦å…§å®¹)")
                return match_pos
            else:
                print(f"   â­ï¸ è·³éä½ç½® {i+1}: {match_pos} (çœ‹èµ·ä¾†æ˜¯ç›®éŒ„)")
        
        # å¦‚æœéƒ½æ²’æ‰¾åˆ°çœŸå¯¦å…§å®¹ï¼Œå˜—è©¦å¾ PART I é–‹å§‹
        part_i_match = re.search(r'PART\s+I\s*[^\w]', content, re.IGNORECASE | re.MULTILINE)
        if part_i_match:
            part_i_pos = part_i_match.end()
            print(f"   ğŸ“ ä½¿ç”¨ PART I ä½ç½®: {part_i_pos}")
            return part_i_pos
        
        # æœ€å¾Œæ‰‹æ®µï¼šå¾æ–‡ä»¶çš„1/3è™•é–‹å§‹
        fallback_pos = len(content) // 3
        print(f"   ğŸ“ ä½¿ç”¨é è¨­ä½ç½®: {fallback_pos} (æª”æ¡ˆç¸½é•·åº¦: {len(content)})")
        return fallback_pos

    def looks_like_real_content(self, text):
        """æª¢æŸ¥æ–‡æœ¬æ˜¯å¦çœ‹èµ·ä¾†åƒçœŸå¯¦å…§å®¹è€Œä¸æ˜¯ç›®éŒ„æˆ–HTML"""
        # æ¸…ç† HTML æ¨™ç±¤å’Œç‰¹æ®Šå­—ç¬¦
        clean_text = re.sub(r'<[^>]+>', ' ', text)
        clean_text = re.sub(r'&#\d+;', ' ', clean_text)  # æ¸…ç†HTMLå¯¦é«”
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        # å¦‚æœå¤ªçŸ­ï¼Œå¯èƒ½ä¸æ˜¯çœŸå¯¦å…§å®¹
        if len(clean_text) < 50:
            return False
        
        # æª¢æŸ¥æ˜¯å¦æ˜é¡¯æ˜¯ç›®éŒ„ï¼ˆåŒ…å«å¤ªå¤šé ç¢¼ï¼‰
        page_number_pattern = r'\b\d{1,3}\b'
        page_numbers = re.findall(page_number_pattern, clean_text)
        if len(page_numbers) > 10:  # å¦‚æœæœ‰å¤ªå¤šæ•¸å­—ï¼Œå¯èƒ½æ˜¯ç›®éŒ„
            return False
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ˜é¡¯çš„ç›®éŒ„ç‰¹å¾µ
        toc_indicators = [
            r'Table\s+of\s+Contents',
            r'Part\s+I.*Part\s+II',
            r'Item\s+\d+\..*\d+\s*$',
            r'Page\s*\d+',
            r'\.{3,}',  # å¤šå€‹é»ï¼ˆç›®éŒ„ä¸­å¸¸è¦‹ï¼‰
        ]
        
        for indicator in toc_indicators:
            if re.search(indicator, clean_text, re.IGNORECASE):
                return False
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«å¯¦è³ªæ€§çš„æ¥­å‹™å…§å®¹é—œéµå­—ï¼ˆé™ä½é–€æª»ï¼‰
        business_keywords = [
            r'Company', r'business', r'operations', r'products', r'services', 
            r'revenue', r'customers', r'markets', r'develops', r'designs', 
            r'manufactures', r'sells', r'technology', r'solutions', r'platform',
            r'segment', r'industry', r'competitive', r'strategy', r'fiscal',
            r'Our Company', r'We are', r'We operate', r'We design', r'We develop'
        ]
        
        keyword_count = 0
        for keyword in business_keywords:
            if re.search(keyword, clean_text, re.IGNORECASE):
                keyword_count += 1
        
        # åªéœ€è¦è‡³å°‘2å€‹ç›¸é—œé—œéµå­—å°±èªç‚ºæ˜¯çœŸå¯¦å…§å®¹ï¼ˆé™ä½é–€æª»ï¼‰
        return keyword_count >= 2

    def clean_html_content(self, content):
        """å¾¹åº•æ¸…ç†HTMLæ¨™ç±¤ã€CSSæ¨£å¼å’Œç‰¹æ®Šå­—ç¬¦ - å„ªåŒ–ç‰ˆæœ¬"""
        stage_start = time.time()
        
        # ä½¿ç”¨é ç·¨è­¯çš„æ­£å‰‡è¡¨é”å¼
        content = self.html_style_pattern.sub(' ', content)
        content = self.html_script_pattern.sub(' ', content)
        content = self.html_tag_pattern.sub(' ', content)
        content = self.html_entity_pattern.sub(' ', content)
        content = self.whitespace_pattern.sub(' ', content)
        
        # ç§»é™¤ä¸å¿…è¦çš„ç‰¹æ®Šå­—ç¬¦
        content = content.replace('\x00', ' ')  # NULL å­—ç¬¦
        content = content.replace('\r\n', '\n').replace('\r', '\n')  # çµ±ä¸€æ›è¡Œç¬¦
        
        stage_duration = time.time() - stage_start
        self.log_performance("HTMLæ¸…ç†", stage_duration)
        
        return content.strip()

    def extract_items(self, content):
        """æå–æ‰€æœ‰Itemå…§å®¹ - æ”¹é€²ç‰ˆï¼Œè™•ç†å°‘æ–¼5å€‹å­—çš„ç›®éŒ„å€å¡Š"""
        extract_start = time.time()
        items = {}
        
        # ç¬¬ä¸€æ­¥ï¼šå¾¹åº•æ¸…ç†HTMLæ¨™ç±¤ã€CSSæ¨£å¼å’Œç‰¹æ®Šå­—ç¬¦
        print("   ğŸ§¹ æ¸…ç†HTMLæ¨™ç±¤å’ŒCSSæ¨£å¼...")
        clean_content = self.clean_html_content(content)
        
        print(f"   ğŸ“Š æ¸…ç†å‰é•·åº¦: {len(content):,}, æ¸…ç†å¾Œé•·åº¦: {len(clean_content):,}")
        
        # æ‰¾åˆ°å¯¦éš›å…§å®¹é–‹å§‹ä½ç½®
        content_start = self.find_content_start_position(clean_content)
        print(f"   ğŸ“ çœŸå¯¦å…§å®¹é–‹å§‹ä½ç½®: {content_start:,}")
        
        # å„ªåŒ–ï¼šä¸€æ¬¡æ€§æ‰¾åˆ°æ‰€æœ‰Itemä½ç½®ï¼Œé¿å…é‡è¤‡æœç´¢
        all_item_positions = {}
        for item_key, pattern in self.compiled_patterns.items():
            if item_key == 'appendix':
                continue
            search_content = clean_content[content_start:]
            matches = list(pattern.finditer(search_content))
            if matches:
                all_item_positions[item_key] = [(m.start() + content_start, m.end() + content_start) for m in matches]
        
        print(f"   ğŸ” æ‰¾åˆ° {len(all_item_positions)} ç¨®Itemé¡å‹çš„åŒ¹é…ä½ç½®")
        
        # è™•ç†ä¸€èˆ¬çš„Items
        for item_key in self.compiled_patterns.keys():
            if item_key == 'appendix':  # é™„éŒ„éœ€è¦ç‰¹æ®Šè™•ç†
                continue
            
            try:
                if item_key not in all_item_positions:
                    items[item_key] = None
                    print(f"   âŒ {item_key}: åœ¨çœŸå¯¦å…§å®¹å€åŸŸæœªæ‰¾åˆ°åŒ¹é…")
                    continue
                
                positions = all_item_positions[item_key]
                found_valid_content = False
                
                for match_idx, (start_pos, end_pos) in enumerate(positions):
                    print(f"   ğŸ” {item_key} ä½ç½® {match_idx + 1}: åœ¨ä½ç½® {start_pos:,} æ‰¾åˆ°åŒ¹é…")
                    
                    # å°æ–¼æŸäº›Itemï¼Œå˜—è©¦æ‰¾åˆ°çœŸæ­£çš„å…§å®¹é–‹å§‹ä½ç½®
                    if item_key == 'item_1':
                        # å°‹æ‰¾ "Business" æ¨™é¡Œï¼ˆå¯èƒ½åœ¨Item 1å¾Œé¢ä¸€æ®µè·é›¢ï¼‰
                        business_pattern = re.compile(r'Business\s*[^\w]', re.IGNORECASE)
                        business_search_range = clean_content[start_pos:start_pos + 1000]
                        business_match = business_pattern.search(business_search_range)
                        if business_match:
                            # å¾Businessæ¨™é¡Œå¾Œé–‹å§‹æå–å…§å®¹
                            content_start_pos = start_pos + business_match.end()
                            print(f"   ğŸ“ {item_key}: æ‰¾åˆ°Businessæ¨™é¡Œï¼Œå¾ä½ç½® {content_start_pos:,} é–‹å§‹æå–")
                        else:
                            # å¦‚æœæ²’æ‰¾åˆ°Businessï¼Œå¾ItemåŒ¹é…ä½ç½®å¾Œé–‹å§‹
                            content_start_pos = end_pos
                    else:
                        content_start_pos = end_pos
                    
                    # å¿«é€ŸæŸ¥æ‰¾ä¸‹ä¸€å€‹Itemçš„é–‹å§‹ä½ç½®ä½œç‚ºçµæŸé»
                    next_item_pos = len(clean_content)
                    for other_key, other_positions in all_item_positions.items():
                        if other_key == item_key:
                            continue
                        for other_start, _ in other_positions:
                            if other_start > content_start_pos and other_start < next_item_pos:
                                next_item_pos = other_start
                    
                    # æå–Itemå…§å®¹
                    item_content = clean_content[content_start_pos:next_item_pos].strip()
                    
                    # å¿«é€Ÿæ¸…ç†å…§å®¹
                    item_content = self.whitespace_pattern.sub(' ', item_content)
                    
                    print(f"   ğŸ“ {item_key} ä½ç½® {match_idx + 1}: æå–äº† {len(item_content):,} å­—ç¬¦")
                    
                    # é—œéµæª¢æŸ¥ï¼šå¦‚æœå…§å®¹å°‘æ–¼5å€‹å­—ç¬¦ï¼Œèªç‚ºé‚„åœ¨ç›®éŒ„å€å¡Šï¼Œç¹¼çºŒå˜—è©¦ä¸‹ä¸€å€‹åŒ¹é…
                    if len(item_content.strip()) < 5:
                        print(f"   â­ï¸ {item_key} ä½ç½® {match_idx + 1}: å…§å®¹å¤ªçŸ­ ({len(item_content)} å­—ç¬¦)ï¼Œå¯èƒ½æ˜¯ç›®éŒ„ï¼Œç¹¼çºŒæœç´¢...")
                        continue
                    
                    # é¡å¤–æª¢æŸ¥ï¼šå¦‚æœå…§å®¹æ˜é¡¯æ˜¯ç›®éŒ„ç‰¹å¾µï¼ˆåªæœ‰æ•¸å­—å’ŒçŸ­è©ï¼‰
                    if self.looks_like_table_of_contents(item_content):
                        print(f"   â­ï¸ {item_key} ä½ç½® {match_idx + 1}: çœ‹èµ·ä¾†æ˜¯ç›®éŒ„å…§å®¹ï¼Œç¹¼çºŒæœç´¢...")
                        continue
                    
                    # æª¢æŸ¥å…§å®¹æ˜¯å¦çœŸçš„æœ‰æ„ç¾©ï¼ˆä¸åªæ˜¯å¼•ç”¨æˆ–è½‰å‘ï¼‰
                    if self.is_meaningful_content(item_content):
                        # æ‰¾åˆ°æœ‰æ•ˆå…§å®¹ï¼Œé™åˆ¶é•·åº¦ä¸¦ä¿å­˜
                        if len(item_content) > 65535:  # TEXT æ¬„ä½é™åˆ¶
                            item_content = item_content[:65532] + "..."
                        
                        items[item_key] = item_content
                        found_valid_content = True
                        
                        # é¡¯ç¤ºæ‰¾åˆ°çš„å…§å®¹é è¦½
                        preview = item_content[:100] + "..." if len(item_content) > 100 else item_content
                        print(f"   âœ… {item_key}: {len(item_content):,} å­—ç¬¦ (ä½ç½® {match_idx + 1}) - {preview}")
                        break  # æ‰¾åˆ°æœ‰æ•ˆå…§å®¹å¾Œè·³å‡ºå¾ªç’°
                    else:
                        print(f"   â­ï¸ {item_key} ä½ç½® {match_idx + 1}: å…§å®¹ä¸å¤ å¯¦è³ªï¼Œç¹¼çºŒæœç´¢...")
                        continue
                
                # å¦‚æœæ‰€æœ‰åŒ¹é…éƒ½ç„¡æ•ˆï¼Œè¨­ç‚ºNone
                if not found_valid_content:
                    items[item_key] = None
                    print(f"   âŒ {item_key}: æ‰€æœ‰ä½ç½®éƒ½ç„¡æœ‰æ•ˆå…§å®¹")
                
            except Exception as e:
                print(f"âš ï¸ æå– {item_key} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                items[item_key] = None
        
        # ç‰¹æ®Šè™•ç†é™„éŒ„
        items['appendix'] = self.extract_appendix(clean_content)
        
        extract_duration = time.time() - extract_start
        self.log_performance("Itemsæå–å®Œæˆ", extract_duration)
        
        return items

    def looks_like_table_of_contents(self, text):
        """æª¢æŸ¥å…§å®¹æ˜¯å¦çœ‹èµ·ä¾†åƒç›®éŒ„"""
        if len(text.strip()) < 20:  # å¤ªçŸ­çš„å…§å®¹å¯èƒ½æ˜¯ç›®éŒ„
            return True
        
        # å°æ–¼é•·å…§å®¹ï¼ˆè¶…é1000å­—ç¬¦ï¼‰ï¼Œæ›´å¯¬é¬†çš„æª¢æŸ¥
        if len(text.strip()) > 1000:
            # æª¢æŸ¥æ˜¯å¦æœ‰æ˜é¡¯çš„ç›®éŒ„ç‰¹å¾µï¼ˆæé«˜é–€æª»ï¼‰
            toc_patterns = [
                r'Table\s+of\s+Contents',
                r'INDEX\s+TO\s+FINANCIAL\s+STATEMENTS',
                r'^\s*Item\s+\d+[A-Z]?\s+[\.]{3,}',  # Item x ....æ ¼å¼
                r'^\s*Page\s+\d+\s*$',  # å–®ç¨çš„é ç¢¼è¡Œ
            ]
            
            strong_toc_count = 0
            for pattern in toc_patterns:
                if re.search(pattern, text, re.IGNORECASE | re.MULTILINE):
                    strong_toc_count += 1
            
            # å°æ–¼é•·å…§å®¹ï¼Œéœ€è¦è‡³å°‘2å€‹å¼·ç›®éŒ„ç‰¹å¾µæ‰èªç‚ºæ˜¯ç›®éŒ„
            if strong_toc_count < 2:
                return False
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å¤ªå¤šçš„é ç¢¼æ•¸å­—ï¼ˆé™ä½é–€æª»ï¼‰
        numbers = re.findall(r'\b\d{1,3}\b', text)
        words = text.split()
        if len(words) > 0 and len(numbers) > len(words) * 0.4:  # æé«˜åˆ°40%é–€æª»
            return True
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ˜é¡¯çš„ç›®éŒ„ç‰¹å¾µ
        toc_patterns = [
            r'\.\.\.',  # ç›®éŒ„ä¸­çš„é»ç·š
            r'Page\s+\d+',  # é ç¢¼
            r'^\s*\d+\s*$',  # åªæœ‰æ•¸å­—çš„è¡Œ
            r'Table\s+of\s+Contents',
            r'^\s*Item\s+\d+[A-Z]?\s+[\.]{2,}',  # Item x ....æ ¼å¼
        ]
        
        toc_pattern_count = 0
        for pattern in toc_patterns:
            if re.search(pattern, text, re.IGNORECASE | re.MULTILINE):
                toc_pattern_count += 1
        
        # éœ€è¦å¤šå€‹ç›®éŒ„ç‰¹å¾µæ‰èªç‚ºæ˜¯ç›®éŒ„
        return toc_pattern_count >= 2

    def extract_appendix(self, content):
        """ç‰¹æ®Šè™•ç†é™„éŒ„å…§å®¹"""
        try:
            # ç°¡åŒ–çš„é™„éŒ„æå–é‚è¼¯
            f_page_pattern = r'F-\d+'
            f_matches = list(re.finditer(f_page_pattern, content))
            
            if not f_matches:
                return None
            
            # å¾ç¬¬ä¸€å€‹F-é ç¢¼é–‹å§‹æå–
            first_f_match = f_matches[0]
            appendix_start_pos = max(0, first_f_match.start() - 500)
            
            # æå–é™„éŒ„å…§å®¹
            appendix_content = content[appendix_start_pos:].strip()
            
            # æ¸…ç†å’Œé™åˆ¶é•·åº¦
            appendix_content = re.sub(r'\s+', ' ', appendix_content)
            if len(appendix_content) > 65535:
                appendix_content = appendix_content[:65532] + "..."
            
            if appendix_content:
                preview = appendix_content[:100] + "..." if len(appendix_content) > 100 else appendix_content
                print(f"   âœ… appendix: {len(appendix_content)} å­—ç¬¦ - {preview}")
                return appendix_content
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ æå–é™„éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    def generate_content_hash(self, content):
        """ç”Ÿæˆå…§å®¹çš„MD5é›œæ¹Šå€¼ï¼Œç”¨æ–¼æª¢æŸ¥é‡è¤‡"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()

    def save_to_database(self, file_path, metadata, items):
        """å„²å­˜åˆ°è³‡æ–™åº«"""
        if not self.db_connection or not self.db_connection.is_connected():
            print("âŒ è³‡æ–™åº«æœªé€£æ¥")
            return False

        try:
            cursor = self.db_connection.cursor()
            
            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„æ–‡ä»¶
            content_hash = self.generate_content_hash(str(metadata) + str(items))
            check_sql = "SELECT id FROM ten_k_filings WHERE content_hash = %s"
            cursor.execute(check_sql, (content_hash,))
            
            if cursor.fetchone():
                print(f"âš ï¸ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³é: {file_path}")
                return True

            # æ’å…¥æ•¸æ“š
            insert_sql = """
                INSERT INTO ten_k_filings (
                    file_name, document_number, company_name, cik, 
                    report_date, filed_date, content_hash,
                    item_1, item_1a, item_1b, item_2, item_3, item_4,
                    item_5, item_6, item_7, item_7a, item_8, item_9,
                    item_9a, item_9b, item_10, item_11, item_12,
                    item_13, item_14, item_15, item_16, appendix,
                    created_at
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW()
                )
            """
            
            values = (
                os.path.basename(file_path),
                metadata.get('document_number'),
                metadata.get('company_name'),  # ç¾åœ¨æ˜¯è‚¡ç¥¨ä»£è™Ÿ
                metadata.get('cik'),
                metadata.get('report_date'),
                metadata.get('filed_date'),
                content_hash,
                items.get('item_1'),
                items.get('item_1a'),
                items.get('item_1b'),
                items.get('item_2'),
                items.get('item_3'),
                items.get('item_4'),
                items.get('item_5'),
                items.get('item_6'),
                items.get('item_7'),
                items.get('item_7a'),
                items.get('item_8'),
                items.get('item_9'),
                items.get('item_9a'),
                items.get('item_9b'),
                items.get('item_10'),
                items.get('item_11'),
                items.get('item_12'),
                items.get('item_13'),
                items.get('item_14'),
                items.get('item_15'),
                items.get('item_16'),
                items.get('appendix')
            )
            
            cursor.execute(insert_sql, values)
            self.db_connection.commit()
            
            print(f"âœ… æˆåŠŸå„²å­˜: {os.path.basename(file_path)}")
            return True
            
        except Error as e:
            print(f"âŒ è³‡æ–™åº«æ’å…¥å¤±æ•—: {e}")
            return False

    def process_10k_file(self, file_path):
        """è™•ç†å–®å€‹10-Kæ–‡ä»¶ - å„ªåŒ–ç‰ˆæœ¬"""
        print(f"\nğŸ“„ è™•ç†æ–‡ä»¶: {os.path.basename(file_path)}")
        file_start_time = time.time()
        
        try:
            # è®€å–æ–‡ä»¶å…§å®¹
            read_start = time.time()
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
                content = file.read()
            read_duration = time.time() - read_start
            self.log_performance(f"è®€å–æ–‡ä»¶ ({len(content):,} å­—ç¬¦)", read_duration)
            
            # æå–åŸºæœ¬è³‡è¨Š
            metadata_start = time.time()
            metadata = self.extract_filing_metadata(content)
            metadata_duration = time.time() - metadata_start
            self.log_performance("æå–å…ƒæ•¸æ“š", metadata_duration)
            
            print(f"   ğŸ“Š è‚¡ç¥¨ä»£è™Ÿ: {metadata.get('company_name', 'N/A')}")
            print(f"   ğŸ“… å ±å‘Šæ—¥æœŸ: {metadata.get('report_date', 'N/A')}")
            
            # æå–Items
            print(f"   ğŸ” æå–Items...")
            items_start = time.time()
            items = self.extract_items(content)
            items_duration = time.time() - items_start
            self.log_performance("æå–Items", items_duration)
            
            # çµ±è¨ˆæœ‰æ•ˆItems
            valid_items = sum(1 for v in items.values() if v is not None)
            print(f"   âœ… æˆåŠŸæå– {valid_items}/{len(items)} å€‹Items")
            
            # å„²å­˜åˆ°è³‡æ–™åº«
            db_start = time.time()
            success = self.save_to_database(file_path, metadata, items)
            db_duration = time.time() - db_start
            self.log_performance("æ•¸æ“šåº«ä¿å­˜", db_duration)
            
            file_total_duration = time.time() - file_start_time
            self.log_performance(f"è™•ç†æ–‡ä»¶å®Œæˆ", file_total_duration)
            
            return success
            
        except Exception as e:
            print(f"âŒ è™•ç†æ–‡ä»¶å¤±æ•—: {e}")
            return False

    def process_ticker_folder(self):
        """è™•ç†æŒ‡å®šè‚¡ç¥¨ä»£è™Ÿè³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰10-Kæ–‡ä»¶"""
        ticker_10k_path = Path(__file__).parent / "downloads" / self.ticker / "10-K"
        
        if not ticker_10k_path.exists():
            print(f"âŒ æ‰¾ä¸åˆ° {self.ticker} 10-Kè³‡æ–™å¤¾: {ticker_10k_path}")
            return False
        
        # ç²å–æ‰€æœ‰.txtæ–‡ä»¶
        txt_files = list(ticker_10k_path.glob("*.txt"))
        
        if not txt_files:
            print(f"âŒ æ²’æœ‰æ‰¾åˆ° {self.ticker} çš„10-Kæ–‡ä»¶")
            return False
        
        print(f"ğŸ” æ‰¾åˆ° {len(txt_files)} å€‹ {self.ticker} çš„10-Kæ–‡ä»¶")
        
        # é€£æ¥è³‡æ–™åº«
        if not self.connect_database():
            return False
        
        success_count = 0
        
        try:
            for file_path in txt_files:
                if self.process_10k_file(file_path):
                    success_count += 1
        
        finally:
            self.disconnect_database()
        
        print(f"\nğŸ‰ {self.ticker} è™•ç†å®Œæˆ! æˆåŠŸ: {success_count}/{len(txt_files)}")
        return success_count > 0

    def is_meaningful_content(self, content):
        """æª¢æŸ¥å…§å®¹æ˜¯å¦æœ‰æ„ç¾©ï¼ˆä¸åªæ˜¯å¼•ç”¨æˆ–è½‰å‘ï¼‰"""
        if len(content.strip()) < 10:  # æ”¾å¯¬é•·åº¦é™åˆ¶
            return False
        
        # æª¢æŸ¥æ˜¯å¦åªæ˜¯å¼•ç”¨å…¶ä»–æ–‡ä»¶ï¼ˆæ›´å¯¬é¬†çš„åˆ¤æ–·ï¼‰
        reference_patterns = [
            r'information.*incorporated.*by reference',
            r'see.*proxy statement',
            r'refer to.*form',
            r'included.*elsewhere',
            r'set forth.*below',
            r'discussed.*in.*note',
        ]
        
        clean_content = content.lower()
        reference_count = 0
        for pattern in reference_patterns:
            if re.search(pattern, clean_content):
                reference_count += 1
        
        # åªæœ‰ç•¶å…§å®¹å¾ˆçŸ­ä¸”ä¸»è¦æ˜¯å¼•ç”¨æ™‚æ‰èªç‚ºç„¡æ„ç¾©
        if reference_count > 0 and len(content.strip()) < 50:
            return False
        
        # æª¢æŸ¥æ˜¯å¦åªåŒ…å«ç„¡æ„ç¾©çš„çŸ­èª
        meaningless_patterns = [
            r'^None\.$',
            r'^Not applicable\.$',
            r'^N/A$',
            r'^\s*-\s*$',
            r'^\s*\d+\s*$'  # åªæœ‰æ•¸å­—
        ]
        
        for pattern in meaningless_patterns:
            if re.match(pattern, content.strip(), re.IGNORECASE):
                return False
        
        # å°æ–¼å…§å®¹é•·åº¦è¶…é100å­—ç¬¦çš„ï¼ŒåŸºæœ¬ä¸Šéƒ½èªç‚ºæ˜¯æœ‰æ„ç¾©çš„
        if len(content.strip()) >= 100:
            return True
        
        # å°æ–¼è¼ƒçŸ­å…§å®¹ï¼Œæª¢æŸ¥æ˜¯å¦åŒ…å«ä¸€äº›å¯¦è³ªæ€§é—œéµå­—
        substantial_keywords = [
            r'Company', r'business', r'operations', r'revenue', r'income',
            r'assets', r'liabilities', r'cash', r'employees', r'products',
            r'services', r'customers', r'markets', r'competition', r'risks',
            r'strategy', r'acquisitions', r'development', r'technology',
            r'headquarters', r'located', r'properties', r'legal', r'proceedings',
            r'management', r'discussion', r'analysis', r'financial', r'statements'
        ]
        
        keyword_count = 0
        for keyword in substantial_keywords:
            if re.search(keyword, content, re.IGNORECASE):
                keyword_count += 1
        
        # é™ä½é—œéµå­—è¦æ±‚ï¼Œåªéœ€è¦1å€‹ç›¸é—œé—œéµå­—å³å¯
        return keyword_count >= 1

def main():
    """ä¸»å‡½æ•¸"""
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python parse_single_stock.py <è‚¡ç¥¨ä»£è™Ÿ>")
        print("ç¯„ä¾‹: python parse_single_stock.py AAPL")
        sys.exit(1)
    
    ticker = sys.argv[1].upper()
    parser = SingleStockTenKParser(ticker)
    
    success = parser.process_ticker_folder()
    
    if success:
        sys.exit(0)
    else:
        sys.exit(1)

if __name__ == "__main__":
    main() 