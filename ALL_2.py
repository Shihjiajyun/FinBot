#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡è™•ç†æ‰€æœ‰è‚¡ç¥¨çš„ 10-K Filing Items Parser
éæ­· downloads è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰å…¬å¸è²¡å ±ï¼Œæ‹†è§£ ITEM ä¸¦å­˜å…¥è³‡æ–™è¡¨
æª¢æŸ¥æ˜¯å¦å·²è¢«æ‹†è§£éï¼Œé¿å…é‡è¤‡è™•ç†
ç‰¹åˆ¥è™•ç†æ–°ä¸‹è¼‰çš„è‚¡ç¥¨ï¼Œç¢ºä¿æ‰€æœ‰è‚¡ç¥¨éƒ½è¢«æ‹†è§£
"""

import os
import re
import sys
import mysql.connector
from mysql.connector import Error
from datetime import datetime
from pathlib import Path
import hashlib
import time

class BatchTenKParser:
    def __init__(self, db_config=None):
        self.start_time = time.time()
        
        # è³‡æ–™åº«é…ç½®
        self.db_config = db_config or {
            'host': '43.207.210.147',
            'database': 'finbot_db',
            'user': 'myuser',
            'password': '123456789',
            'charset': 'utf8mb4'
        }
        
        self.db_connection = None
        
        # ç·¨è­¯æ­£å‰‡è¡¨é”å¼ä»¥æé«˜æ€§èƒ½
        self.compiled_patterns = {}
        raw_patterns = {
            'item_1': r'Item\s+1\.\s*(?:Business|BUSINESS|$)',
            'item_1a': r'Item\s+1A\.\s*(?:Risk Factors|RISK FACTORS|$)',
            'item_1b': r'Item\s+1B\.\s*(?:Unresolved Staff Comments|UNRESOLVED STAFF COMMENTS|$)',
            'item_2': r'Item\s+2\.\s*(?:Properties|PROPERTIES|$)',
            'item_3': r'Item\s+3\.\s*(?:Legal Proceedings|LEGAL PROCEEDINGS|$)',
            'item_4': r'Item\s+4\.\s*(?:Mine Safety|MINE SAFETY|$)',
            'item_5': r'Item\s+5\.\s*(?:Market for Registrant|MARKET FOR REGISTRANT|$)',
            'item_6': r'Item\s+6\.\s*(?:Selected Financial Data|SELECTED FINANCIAL DATA|$)',
            'item_7': r'Item\s+7\.\s*(?:Management|MANAGEMENT|$)',
            'item_7a': r'Item\s+7A\.\s*(?:Quantitative and Qualitative|QUANTITATIVE AND QUALITATIVE|$)',
            'item_8': r'Item\s+8\.\s*(?:Financial Statements|FINANCIAL STATEMENTS|$)',
            'item_9': r'Item\s+9\.\s*(?:Changes in and Disagreements|CHANGES IN AND DISAGREEMENTS|$)',
            'item_9a': r'Item\s+9A\.\s*(?:Controls and Procedures|CONTROLS AND PROCEDURES|$)',
            'item_9b': r'Item\s+9B\.\s*(?:Other Information|OTHER INFORMATION|$)',
            'item_10': r'Item\s+10\.\s*(?:Directors|DIRECTORS|$)',
            'item_11': r'Item\s+11\.\s*(?:Executive Compensation|EXECUTIVE COMPENSATION|$)',
            'item_12': r'Item\s+12\.\s*(?:Security Ownership|SECURITY OWNERSHIP|$)',
            'item_13': r'Item\s+13\.\s*(?:Certain Relationships|CERTAIN RELATIONSHIPS|$)',
            'item_14': r'Item\s+14\.\s*(?:Principal Accountant|PRINCIPAL ACCOUNTANT|$)',
            'item_15': r'Item\s+15\.\s*(?:Exhibits|EXHIBITS|$)',
            'item_16': r'Item\s+16\.\s*(?:Form 10-K Summary|FORM 10-K SUMMARY|$)',
            'appendix': r'(?:INDEX\s+TO\s+FINANCIAL\s+STATEMENTS|APPENDIX|CONSOLIDATED\s+FINANCIAL\s+STATEMENTS)'
        }
        
        # é ç·¨è­¯æ­£å‰‡è¡¨é”å¼
        print("ğŸš€ é ç·¨è­¯æ­£å‰‡è¡¨é”å¼ä»¥æé«˜æ€§èƒ½...")
        for key, pattern in raw_patterns.items():
            self.compiled_patterns[key] = re.compile(pattern, re.IGNORECASE | re.MULTILINE)
        
        # å¸¸ç”¨çš„æ¸…ç†æ¨¡å¼ä¹Ÿé ç·¨è­¯
        self.html_style_pattern = re.compile(r'<style[^>]*>.*?</style>', re.DOTALL | re.IGNORECASE)
        self.html_script_pattern = re.compile(r'<script[^>]*>.*?</script>', re.DOTALL | re.IGNORECASE)
        self.html_tag_pattern = re.compile(r'<[^>]+>')
        self.html_entity_pattern = re.compile(r'&#\d+;')
        self.whitespace_pattern = re.compile(r'\s+')
        
        print("âœ… åˆå§‹åŒ–å®Œæˆ")

    def log_performance(self, stage, duration=None):
        """è¨˜éŒ„æ€§èƒ½è³‡è¨Š"""
        current_time = time.time()
        total_elapsed = current_time - self.start_time
        if duration is None:
            print(f"â±ï¸ [{stage}] ç¸½è€—æ™‚: {total_elapsed:.2f}ç§’")
        else:
            print(f"â±ï¸ [{stage}] æœ¬éšæ®µ: {duration:.2f}ç§’, ç¸½è€—æ™‚: {total_elapsed:.2f}ç§’")

    def connect_database(self):
        """é€£æ¥è³‡æ–™åº«"""
        try:
            self.db_connection = mysql.connector.connect(**self.db_config)
            if self.db_connection.is_connected():
                print("âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
                return True
        except Error as e:
            print(f"âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
            return False

    def disconnect_database(self):
        """æ–·é–‹è³‡æ–™åº«é€£æ¥"""
        if self.db_connection and self.db_connection.is_connected():
            self.db_connection.close()
            print("âœ… è³‡æ–™åº«é€£æ¥å·²é—œé–‰")

    def check_ticker_already_parsed(self, ticker):
        """æª¢æŸ¥è‚¡ç¥¨æ˜¯å¦å·²ç¶“è¢«æ‹†è§£é"""
        try:
            cursor = self.db_connection.cursor()
            cursor.execute("SELECT COUNT(*) FROM ten_k_filings WHERE company_name = %s", (ticker,))
            count = cursor.fetchone()[0]
            cursor.close()
            return count > 0
        except Error as e:
            print(f"âŒ æª¢æŸ¥æ‹†è§£ç‹€æ…‹å¤±æ•—: {e}")
            return False

    def extract_filing_metadata(self, content, ticker):
        """æå–10-Kå ±å‘Šçš„åŸºæœ¬è³‡è¨Š"""
        metadata = {}
        
        # æå–æ–‡ä»¶ç·¨è™Ÿ
        doc_number_pattern = r'DOCUMENT\s+(\d{10}-\d{2}-\d{6})'
        doc_match = re.search(doc_number_pattern, content)
        metadata['document_number'] = doc_match.group(1) if doc_match else None
        
        # ä½¿ç”¨è‚¡ç¥¨ä»£è™Ÿä½œç‚ºå…¬å¸åç¨±
        metadata['company_name'] = ticker
        
        # æå–CIK
        cik_pattern = r'CENTRAL INDEX KEY:\s+(\d+)'
        cik_match = re.search(cik_pattern, content)
        metadata['cik'] = cik_match.group(1) if cik_match else None
        
        # æå–å ±å‘Šæ—¥æœŸ
        date_pattern = r'CONFORMED PERIOD OF REPORT:\s+(\d{8})'
        date_match = re.search(date_pattern, content)
        if date_match:
            date_str = date_match.group(1)
            metadata['report_date'] = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        else:
            metadata['report_date'] = None
        
        # æå–æäº¤æ—¥æœŸ
        filed_pattern = r'FILED AS OF DATE:\s+(\d{8})'
        filed_match = re.search(filed_pattern, content)
        if filed_match:
            filed_str = filed_match.group(1)
            metadata['filed_date'] = f"{filed_str[:4]}-{filed_str[4:6]}-{filed_str[6:8]}"
        else:
            metadata['filed_date'] = None
        
        return metadata

    def find_content_start_position(self, content):
        """æ‰¾åˆ°å¯¦éš›å…§å®¹é–‹å§‹ä½ç½®ï¼Œè·³éTABLE OF CONTENTS"""
        
        # ç›´æ¥å°‹æ‰¾æ‰€æœ‰ Item 1 çš„ä½ç½®ï¼Œç„¶å¾Œé¸æ“‡çœŸæ­£åŒ…å«å…§å®¹çš„é‚£ä¸€å€‹
        item1_matches = list(re.finditer(r'Item\s+1\.', content, re.IGNORECASE | re.MULTILINE))
        
        for i, match in enumerate(item1_matches):
            match_pos = match.start()
            # æª¢æŸ¥é€™å€‹ä½ç½®å¾Œé¢æ›´å¤§ç¯„åœçš„å…§å®¹ï¼ˆæé«˜åˆ°3000å­—ç¬¦ï¼‰
            sample_content = content[match_pos:match_pos + 3000]
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«çœŸå¯¦çš„æ¥­å‹™å…§å®¹
            if self.looks_like_real_content(sample_content):
                return match_pos
        
        # å¦‚æœéƒ½æ²’æ‰¾åˆ°çœŸå¯¦å…§å®¹ï¼Œå˜—è©¦å¾ PART I é–‹å§‹
        part_i_match = re.search(r'PART\s+I\s*[^\w]', content, re.IGNORECASE | re.MULTILINE)
        if part_i_match:
            part_i_pos = part_i_match.end()
            return part_i_pos
        
        # æœ€å¾Œæ‰‹æ®µï¼šå¾æ–‡ä»¶çš„1/3è™•é–‹å§‹
        fallback_pos = len(content) // 3
        return fallback_pos

    def looks_like_real_content(self, text):
        """æª¢æŸ¥æ–‡æœ¬æ˜¯å¦çœ‹èµ·ä¾†åƒçœŸå¯¦å…§å®¹è€Œä¸æ˜¯ç›®éŒ„æˆ–HTML"""
        # æ¸…ç† HTML æ¨™ç±¤å’Œç‰¹æ®Šå­—ç¬¦
        clean_text = re.sub(r'<[^>]+>', ' ', text)
        clean_text = re.sub(r'&#\d+;', ' ', clean_text)  # æ¸…ç†HTMLå¯¦é«”
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        # å¦‚æœå¤ªçŸ­ï¼Œå¯èƒ½ä¸æ˜¯çœŸå¯¦å…§å®¹
        if len(clean_text) < 50:
            return False
        
        # æª¢æŸ¥æ˜¯å¦æ˜é¡¯æ˜¯ç›®éŒ„ï¼ˆåŒ…å«å¤ªå¤šé ç¢¼ï¼‰
        page_number_pattern = r'\b\d{1,3}\b'
        page_numbers = re.findall(page_number_pattern, clean_text)
        if len(page_numbers) > 10:  # å¦‚æœæœ‰å¤ªå¤šæ•¸å­—ï¼Œå¯èƒ½æ˜¯ç›®éŒ„
            return False
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ˜é¡¯çš„ç›®éŒ„ç‰¹å¾µ
        toc_indicators = [
            r'Table\s+of\s+Contents',
            r'Part\s+I.*Part\s+II',
            r'Item\s+\d+\..*\d+\s*$',
            r'Page\s*\d+',
            r'\.{3,}',  # å¤šå€‹é»ï¼ˆç›®éŒ„ä¸­å¸¸è¦‹ï¼‰
        ]
        
        for indicator in toc_indicators:
            if re.search(indicator, clean_text, re.IGNORECASE):
                return False
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«å¯¦è³ªæ€§çš„æ¥­å‹™å…§å®¹é—œéµå­—ï¼ˆé™ä½é–€æª»ï¼‰
        business_keywords = [
            r'Company', r'business', r'operations', r'products', r'services', 
            r'revenue', r'customers', r'markets', r'develops', r'designs', 
            r'manufactures', r'sells', r'technology', r'solutions', r'platform',
            r'segment', r'industry', r'competitive', r'strategy', r'fiscal',
            r'Our Company', r'We are', r'We operate', r'We design', r'We develop'
        ]
        
        keyword_count = 0
        for keyword in business_keywords:
            if re.search(keyword, clean_text, re.IGNORECASE):
                keyword_count += 1
        
        # åªéœ€è¦è‡³å°‘2å€‹ç›¸é—œé—œéµå­—å°±èªç‚ºæ˜¯çœŸå¯¦å…§å®¹ï¼ˆé™ä½é–€æª»ï¼‰
        return keyword_count >= 2

    def clean_html_content(self, content):
        """æ¸…ç†HTMLå…§å®¹ï¼Œä¿ç•™æ–‡æœ¬"""
        # ç§»é™¤æ¨£å¼å’Œè…³æœ¬
        content = self.html_style_pattern.sub('', content)
        content = self.html_script_pattern.sub('', content)
        
        # ç§»é™¤HTMLæ¨™ç±¤
        content = self.html_tag_pattern.sub(' ', content)
        
        # æ¸…ç†HTMLå¯¦é«”
        content = self.html_entity_pattern.sub(' ', content)
        
        # è¦ç¯„åŒ–ç©ºç™½å­—ç¬¦
        content = self.whitespace_pattern.sub(' ', content)
        
        return content.strip()

    def extract_items(self, content):
        """æå–æ‰€æœ‰Itemå…§å®¹ - æ”¹é€²ç‰ˆï¼Œè™•ç†å°‘æ–¼5å€‹å­—çš„ç›®éŒ„å€å¡Š"""
        extract_start = time.time()
        items = {}
        
        # ç¬¬ä¸€æ­¥ï¼šå¾¹åº•æ¸…ç†HTMLæ¨™ç±¤ã€CSSæ¨£å¼å’Œç‰¹æ®Šå­—ç¬¦
        print("   ğŸ§¹ æ¸…ç†HTMLæ¨™ç±¤å’ŒCSSæ¨£å¼...")
        clean_content = self.clean_html_content(content)
        
        print(f"   ğŸ“Š æ¸…ç†å‰é•·åº¦: {len(content):,}, æ¸…ç†å¾Œé•·åº¦: {len(clean_content):,}")
        
        # æ‰¾åˆ°å¯¦éš›å…§å®¹é–‹å§‹ä½ç½®
        content_start = self.find_content_start_position(clean_content)
        print(f"   ğŸ“ çœŸå¯¦å…§å®¹é–‹å§‹ä½ç½®: {content_start:,}")
        
        # å„ªåŒ–ï¼šä¸€æ¬¡æ€§æ‰¾åˆ°æ‰€æœ‰Itemä½ç½®ï¼Œé¿å…é‡è¤‡æœç´¢
        all_item_positions = {}
        for item_key, pattern in self.compiled_patterns.items():
            if item_key == 'appendix':
                continue
            search_content = clean_content[content_start:]
            matches = list(pattern.finditer(search_content))
            if matches:
                all_item_positions[item_key] = [(m.start() + content_start, m.end() + content_start) for m in matches]
        
        print(f"   ğŸ” æ‰¾åˆ° {len(all_item_positions)} ç¨®Itemé¡å‹çš„åŒ¹é…ä½ç½®")
        
        # è™•ç†ä¸€èˆ¬çš„Items
        for item_key in self.compiled_patterns.keys():
            if item_key == 'appendix':  # é™„éŒ„éœ€è¦ç‰¹æ®Šè™•ç†
                continue
            
            try:
                if item_key not in all_item_positions:
                    items[item_key] = None
                    print(f"   âŒ {item_key}: åœ¨çœŸå¯¦å…§å®¹å€åŸŸæœªæ‰¾åˆ°åŒ¹é…")
                    continue
                
                positions = all_item_positions[item_key]
                found_valid_content = False
                
                for match_idx, (start_pos, end_pos) in enumerate(positions):
                    print(f"   ğŸ” {item_key} ä½ç½® {match_idx + 1}: åœ¨ä½ç½® {start_pos:,} æ‰¾åˆ°åŒ¹é…")
                    
                    # å°æ–¼æŸäº›Itemï¼Œå˜—è©¦æ‰¾åˆ°çœŸæ­£çš„å…§å®¹é–‹å§‹ä½ç½®
                    if item_key == 'item_1':
                        # å°‹æ‰¾ "Business" æ¨™é¡Œï¼ˆå¯èƒ½åœ¨Item 1å¾Œé¢ä¸€æ®µè·é›¢ï¼‰
                        business_pattern = re.compile(r'Business\s*[^\w]', re.IGNORECASE)
                        business_search_range = clean_content[start_pos:start_pos + 1000]
                        business_match = business_pattern.search(business_search_range)
                        if business_match:
                            # å¾Businessæ¨™é¡Œå¾Œé–‹å§‹æå–å…§å®¹
                            content_start_pos = start_pos + business_match.end()
                            print(f"   ğŸ“ {item_key}: æ‰¾åˆ°Businessæ¨™é¡Œï¼Œå¾ä½ç½® {content_start_pos:,} é–‹å§‹æå–")
                        else:
                            # å¦‚æœæ²’æ‰¾åˆ°Businessï¼Œå¾ItemåŒ¹é…ä½ç½®å¾Œé–‹å§‹
                            content_start_pos = end_pos
                    else:
                        content_start_pos = end_pos
                    
                    # å¿«é€ŸæŸ¥æ‰¾ä¸‹ä¸€å€‹Itemçš„é–‹å§‹ä½ç½®ä½œç‚ºçµæŸé»
                    next_item_pos = len(clean_content)
                    for other_key, other_positions in all_item_positions.items():
                        if other_key == item_key:
                            continue
                        for other_start, _ in other_positions:
                            if other_start > content_start_pos and other_start < next_item_pos:
                                next_item_pos = other_start
                    
                    # æå–Itemå…§å®¹
                    item_content = clean_content[content_start_pos:next_item_pos].strip()
                    
                    # å¿«é€Ÿæ¸…ç†å…§å®¹
                    item_content = self.whitespace_pattern.sub(' ', item_content)
                    
                    print(f"   ğŸ“ {item_key} ä½ç½® {match_idx + 1}: æå–äº† {len(item_content):,} å­—ç¬¦")
                    
                    # é—œéµæª¢æŸ¥ï¼šå¦‚æœå…§å®¹å°‘æ–¼5å€‹å­—ç¬¦ï¼Œèªç‚ºé‚„åœ¨ç›®éŒ„å€å¡Šï¼Œç¹¼çºŒå˜—è©¦ä¸‹ä¸€å€‹åŒ¹é…
                    if len(item_content.strip()) < 5:
                        print(f"   â­ï¸ {item_key} ä½ç½® {match_idx + 1}: å…§å®¹å¤ªçŸ­ ({len(item_content)} å­—ç¬¦)ï¼Œå¯èƒ½æ˜¯ç›®éŒ„ï¼Œç¹¼çºŒæœç´¢...")
                        continue
                    
                    # é¡å¤–æª¢æŸ¥ï¼šå¦‚æœå…§å®¹æ˜é¡¯æ˜¯ç›®éŒ„ç‰¹å¾µï¼ˆåªæœ‰æ•¸å­—å’ŒçŸ­è©ï¼‰
                    if self.looks_like_table_of_contents(item_content):
                        print(f"   â­ï¸ {item_key} ä½ç½® {match_idx + 1}: çœ‹èµ·ä¾†æ˜¯ç›®éŒ„å…§å®¹ï¼Œç¹¼çºŒæœç´¢...")
                        continue
                    
                    # æª¢æŸ¥å…§å®¹æ˜¯å¦çœŸçš„æœ‰æ„ç¾©ï¼ˆä¸åªæ˜¯å¼•ç”¨æˆ–è½‰å‘ï¼‰
                    if self.is_meaningful_content(item_content):
                        # æ‰¾åˆ°æœ‰æ•ˆå…§å®¹ï¼Œé™åˆ¶é•·åº¦ä¸¦ä¿å­˜
                        if len(item_content) > 65535:  # TEXT æ¬„ä½é™åˆ¶
                            item_content = item_content[:65532] + "..."
                        
                        items[item_key] = item_content
                        found_valid_content = True
                        
                        # é¡¯ç¤ºæ‰¾åˆ°çš„å…§å®¹é è¦½
                        preview = item_content[:100] + "..." if len(item_content) > 100 else item_content
                        print(f"   âœ… {item_key}: {len(item_content):,} å­—ç¬¦ (ä½ç½® {match_idx + 1}) - {preview}")
                        break  # æ‰¾åˆ°æœ‰æ•ˆå…§å®¹å¾Œè·³å‡ºå¾ªç’°
                    else:
                        print(f"   â­ï¸ {item_key} ä½ç½® {match_idx + 1}: å…§å®¹ä¸å¤ å¯¦è³ªï¼Œç¹¼çºŒæœç´¢...")
                        continue
                
                # å¦‚æœæ‰€æœ‰åŒ¹é…éƒ½ç„¡æ•ˆï¼Œè¨­ç‚ºNone
                if not found_valid_content:
                    items[item_key] = None
                    print(f"   âŒ {item_key}: æ‰€æœ‰ä½ç½®éƒ½ç„¡æœ‰æ•ˆå…§å®¹")
                
            except Exception as e:
                print(f"âš ï¸ æå– {item_key} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                items[item_key] = None
        
        # ç‰¹æ®Šè™•ç†é™„éŒ„
        items['appendix'] = self.extract_appendix(clean_content)
        
        extract_duration = time.time() - extract_start
        self.log_performance("Itemsæå–å®Œæˆ", extract_duration)
        
        return items

    def looks_like_table_of_contents(self, text):
        """æª¢æŸ¥å…§å®¹æ˜¯å¦çœ‹èµ·ä¾†åƒç›®éŒ„"""
        if len(text.strip()) < 20:  # å¤ªçŸ­çš„å…§å®¹å¯èƒ½æ˜¯ç›®éŒ„
            return True
        
        # å°æ–¼é•·å…§å®¹ï¼ˆè¶…é1000å­—ç¬¦ï¼‰ï¼Œæ›´å¯¬é¬†çš„æª¢æŸ¥
        if len(text.strip()) > 1000:
            # æª¢æŸ¥æ˜¯å¦æœ‰æ˜é¡¯çš„ç›®éŒ„ç‰¹å¾µï¼ˆæé«˜é–€æª»ï¼‰
            toc_patterns = [
                r'Table\s+of\s+Contents',
                r'INDEX\s+TO\s+FINANCIAL\s+STATEMENTS',
                r'^\s*Item\s+\d+[A-Z]?\s+[\.]{3,}',  # Item x ....æ ¼å¼
                r'^\s*Page\s+\d+\s*$',  # å–®ç¨çš„é ç¢¼è¡Œ
            ]
            
            strong_toc_count = 0
            for pattern in toc_patterns:
                if re.search(pattern, text, re.IGNORECASE | re.MULTILINE):
                    strong_toc_count += 1
            
            # å°æ–¼é•·å…§å®¹ï¼Œéœ€è¦è‡³å°‘2å€‹å¼·ç›®éŒ„ç‰¹å¾µæ‰èªç‚ºæ˜¯ç›®éŒ„
            if strong_toc_count < 2:
                return False
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å¤ªå¤šçš„é ç¢¼æ•¸å­—ï¼ˆé™ä½é–€æª»ï¼‰
        numbers = re.findall(r'\b\d{1,3}\b', text)
        words = text.split()
        if len(words) > 0 and len(numbers) > len(words) * 0.4:  # æé«˜åˆ°40%é–€æª»
            return True
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ˜é¡¯çš„ç›®éŒ„ç‰¹å¾µ
        toc_patterns = [
            r'\.\.\.',  # ç›®éŒ„ä¸­çš„é»ç·š
            r'Page\s+\d+',  # é ç¢¼
            r'^\s*\d+\s*$',  # åªæœ‰æ•¸å­—çš„è¡Œ
            r'Table\s+of\s+Contents',
            r'^\s*Item\s+\d+[A-Z]?\s+[\.]{2,}',  # Item x ....æ ¼å¼
        ]
        
        toc_pattern_count = 0
        for pattern in toc_patterns:
            if re.search(pattern, text, re.IGNORECASE | re.MULTILINE):
                toc_pattern_count += 1
        
        # éœ€è¦å¤šå€‹ç›®éŒ„ç‰¹å¾µæ‰èªç‚ºæ˜¯ç›®éŒ„
        return toc_pattern_count >= 2

    def extract_appendix(self, content):
        """ç‰¹æ®Šè™•ç†é™„éŒ„å…§å®¹"""
        try:
            # ç°¡åŒ–çš„é™„éŒ„æå–é‚è¼¯
            f_page_pattern = r'F-\d+'
            f_matches = list(re.finditer(f_page_pattern, content))
            
            if not f_matches:
                return None
            
            # å¾ç¬¬ä¸€å€‹F-é ç¢¼é–‹å§‹æå–
            first_f_match = f_matches[0]
            appendix_start_pos = max(0, first_f_match.start() - 500)
            
            # æå–é™„éŒ„å…§å®¹
            appendix_content = content[appendix_start_pos:].strip()
            
            # æ¸…ç†å’Œé™åˆ¶é•·åº¦
            appendix_content = re.sub(r'\s+', ' ', appendix_content)
            if len(appendix_content) > 65535:
                appendix_content = appendix_content[:65532] + "..."
            
            if appendix_content:
                preview = appendix_content[:100] + "..." if len(appendix_content) > 100 else appendix_content
                print(f"   âœ… appendix: {len(appendix_content)} å­—ç¬¦ - {preview}")
                return appendix_content
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ æå–é™„éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    def generate_content_hash(self, content):
        """ç”Ÿæˆå…§å®¹é›œæ¹Šå€¼"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()

    def save_to_database(self, file_path, metadata, items):
        """å„²å­˜è§£æçµæœåˆ°è³‡æ–™åº«"""
        try:
            cursor = self.db_connection.cursor()
            
            # ç”Ÿæˆå…§å®¹é›œæ¹Šå€¼
            content_for_hash = f"{metadata.get('document_number', '')}{metadata.get('report_date', '')}"
            content_hash = self.generate_content_hash(content_for_hash)
            
            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå…§å®¹
            cursor.execute("SELECT id FROM ten_k_filings WHERE content_hash = %s", (content_hash,))
            if cursor.fetchone():
                print(f"   âš ï¸ è·³éé‡è¤‡å…§å®¹: {os.path.basename(file_path)}")
                cursor.close()
                return True
            
            insert_sql = """
            INSERT INTO ten_k_filings (
                file_name, document_number, company_name, cik, report_date, filed_date, content_hash,
                item_1, item_1a, item_1b, item_2, item_3, item_4, item_5, item_6, item_7, item_7a,
                item_8, item_9, item_9a, item_9b, item_10, item_11, item_12, item_13, item_14, item_15, item_16, appendix
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            values = (
                os.path.basename(file_path),
                metadata.get('document_number'),
                metadata.get('company_name'),  # ç¾åœ¨æ˜¯è‚¡ç¥¨ä»£è™Ÿ
                metadata.get('cik'),
                metadata.get('report_date'),
                metadata.get('filed_date'),
                content_hash,
                items.get('item_1'),
                items.get('item_1a'),
                items.get('item_1b'),
                items.get('item_2'),
                items.get('item_3'),
                items.get('item_4'),
                items.get('item_5'),
                items.get('item_6'),
                items.get('item_7'),
                items.get('item_7a'),
                items.get('item_8'),
                items.get('item_9'),
                items.get('item_9a'),
                items.get('item_9b'),
                items.get('item_10'),
                items.get('item_11'),
                items.get('item_12'),
                items.get('item_13'),
                items.get('item_14'),
                items.get('item_15'),
                items.get('item_16'),
                items.get('appendix')
            )
            
            cursor.execute(insert_sql, values)
            self.db_connection.commit()
            cursor.close()
            
            print(f"âœ… æˆåŠŸå„²å­˜: {os.path.basename(file_path)}")
            return True
            
        except Error as e:
            print(f"âŒ è³‡æ–™åº«æ’å…¥å¤±æ•—: {e}")
            return False

    def process_10k_file(self, file_path, ticker):
        """è™•ç†å–®å€‹10-Kæ–‡ä»¶"""
        print(f"\nğŸ“„ è™•ç†æ–‡ä»¶: {os.path.basename(file_path)}")
        file_start_time = time.time()
        
        try:
            # è®€å–æ–‡ä»¶å…§å®¹
            read_start = time.time()
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
                content = file.read()
            read_duration = time.time() - read_start
            
            # æå–åŸºæœ¬è³‡è¨Š
            metadata_start = time.time()
            metadata = self.extract_filing_metadata(content, ticker)
            metadata_duration = time.time() - metadata_start
            
            print(f"   ğŸ“Š è‚¡ç¥¨ä»£è™Ÿ: {metadata.get('company_name', 'N/A')}")
            print(f"   ğŸ“… å ±å‘Šæ—¥æœŸ: {metadata.get('report_date', 'N/A')}")
            
            # æå–Items
            print(f"   ğŸ” æå–Items...")
            items_start = time.time()
            items = self.extract_items(content)
            items_duration = time.time() - items_start
            
            # çµ±è¨ˆæœ‰æ•ˆItems
            valid_items = sum(1 for v in items.values() if v is not None)
            print(f"   âœ… æˆåŠŸæå– {valid_items}/{len(items)} å€‹Items")
            
            # å„²å­˜åˆ°è³‡æ–™åº«
            db_start = time.time()
            success = self.save_to_database(file_path, metadata, items)
            db_duration = time.time() - db_start
            
            file_total_duration = time.time() - file_start_time
            
            return success
            
        except Exception as e:
            print(f"âŒ è™•ç†æ–‡ä»¶å¤±æ•—: {e}")
            return False

    def process_ticker_folder(self, ticker):
        """è™•ç†æŒ‡å®šè‚¡ç¥¨ä»£è™Ÿè³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰10-Kæ–‡ä»¶"""
        ticker_10k_path = Path(__file__).parent / "downloads" / ticker / "10-K"
        
        if not ticker_10k_path.exists():
            print(f"âŒ æ‰¾ä¸åˆ° {ticker} 10-Kè³‡æ–™å¤¾: {ticker_10k_path}")
            return 0
        
        # ç²å–æ‰€æœ‰.txtæ–‡ä»¶
        txt_files = list(ticker_10k_path.glob("*.txt"))
        
        if not txt_files:
            print(f"âŒ æ²’æœ‰æ‰¾åˆ° {ticker} çš„10-Kæ–‡ä»¶")
            return 0
        
        print(f"ğŸ” æ‰¾åˆ° {len(txt_files)} å€‹ {ticker} çš„10-Kæ–‡ä»¶")
        
        success_count = 0
        
        for file_path in txt_files:
            if self.process_10k_file(file_path, ticker):
                success_count += 1
        
        print(f"ğŸ‰ {ticker} è™•ç†å®Œæˆ! æˆåŠŸ: {success_count}/{len(txt_files)}")
        return success_count

    def is_meaningful_content(self, content):
        """æª¢æŸ¥å…§å®¹æ˜¯å¦æœ‰æ„ç¾©ï¼ˆä¸åªæ˜¯å¼•ç”¨æˆ–è½‰å‘ï¼‰"""
        if len(content.strip()) < 10:
            return False
        
        # æª¢æŸ¥æ˜¯å¦åªæ˜¯å¼•ç”¨å…¶ä»–æ–‡ä»¶ï¼ˆæ›´å¯¬é¬†çš„åˆ¤æ–·ï¼‰
        reference_patterns = [
            r'information.*incorporated.*by reference',
            r'see.*proxy statement',
            r'refer to.*form',
            r'included.*elsewhere',
            r'set forth.*below',
            r'discussed.*in.*note',
        ]
        
        clean_content = content.lower()
        reference_count = 0
        for pattern in reference_patterns:
            if re.search(pattern, clean_content):
                reference_count += 1
        
        # åªæœ‰ç•¶å…§å®¹å¾ˆçŸ­ä¸”ä¸»è¦æ˜¯å¼•ç”¨æ™‚æ‰èªç‚ºç„¡æ„ç¾©
        if reference_count > 0 and len(content.strip()) < 50:
            return False
        
        # æª¢æŸ¥æ˜¯å¦åªåŒ…å«ç„¡æ„ç¾©çš„çŸ­èª
        meaningless_patterns = [
            r'^None\.$',
            r'^Not applicable\.$',
            r'^N/A$',
            r'^\s*-\s*$',
            r'^\s*\d+\s*$'  # åªæœ‰æ•¸å­—
        ]
        
        for pattern in meaningless_patterns:
            if re.match(pattern, content.strip(), re.IGNORECASE):
                return False
        
        # å°æ–¼å…§å®¹é•·åº¦è¶…é100å­—ç¬¦çš„ï¼ŒåŸºæœ¬ä¸Šéƒ½èªç‚ºæ˜¯æœ‰æ„ç¾©çš„
        if len(content.strip()) >= 100:
            return True
        
        # å°æ–¼è¼ƒçŸ­å…§å®¹ï¼Œæª¢æŸ¥æ˜¯å¦åŒ…å«ä¸€äº›å¯¦è³ªæ€§é—œéµå­—
        substantial_keywords = [
            r'Company', r'business', r'operations', r'revenue', r'income',
            r'assets', r'liabilities', r'cash', r'employees', r'products',
            r'services', r'customers', r'markets', r'competition', r'risks',
            r'strategy', r'acquisitions', r'development', r'technology',
            r'headquarters', r'located', r'properties', r'legal', r'proceedings',
            r'management', r'discussion', r'analysis', r'financial', r'statements'
        ]
        
        keyword_count = 0
        for keyword in substantial_keywords:
            if re.search(keyword, content, re.IGNORECASE):
                keyword_count += 1
        
        # é™ä½é—œéµå­—è¦æ±‚ï¼Œåªéœ€è¦1å€‹ç›¸é—œé—œéµå­—å³å¯
        return keyword_count >= 1

    def get_unprocessed_tickers(self):
        """ç²å–å°šæœªæ‹†è§£çš„è‚¡ç¥¨æ¸…å–®"""
        downloads_path = Path(__file__).parent / "downloads"
        
        if not downloads_path.exists():
            return []
        
        # ç²å–æ‰€æœ‰æœ‰10-Kæª”æ¡ˆçš„è‚¡ç¥¨ç›®éŒ„
        ticker_dirs = []
        for item in downloads_path.iterdir():
            if item.is_dir():
                ten_k_path = item / "10-K"
                if ten_k_path.exists():
                    txt_files = list(ten_k_path.glob("*.txt"))
                    if txt_files:  # å¦‚æœæœ‰txtæª”æ¡ˆ
                        ticker_dirs.append(item)
        
        # é€£æ¥è³‡æ–™åº«æª¢æŸ¥å“ªäº›é‚„æ²’è¢«æ‹†è§£
        if not self.connect_database():
            return []
        
        unprocessed_tickers = []
        for ticker_dir in ticker_dirs:
            ticker = ticker_dir.name.upper()
            if not self.check_ticker_already_parsed(ticker):
                unprocessed_tickers.append((ticker, ticker_dir))
        
        self.disconnect_database()
        return unprocessed_tickers

    def process_all_tickers(self):
        """è™•ç†æ‰€æœ‰è‚¡ç¥¨çš„è²¡å ±"""
        downloads_path = Path(__file__).parent / "downloads"
        
        if not downloads_path.exists():
            print(f"âŒ æ‰¾ä¸åˆ° downloads è³‡æ–™å¤¾: {downloads_path}")
            return False
        
        # ç²å–å°šæœªæ‹†è§£çš„è‚¡ç¥¨æ¸…å–®
        unprocessed_tickers = self.get_unprocessed_tickers()
        
        if not unprocessed_tickers:
            print("ğŸ‰ æ‰€æœ‰è‚¡ç¥¨çš„10-Kè²¡å ±éƒ½å·²ç¶“æ‹†è§£å®Œæˆ!")
            return True
        
        print(f"ğŸ¯ æ‰¾åˆ° {len(unprocessed_tickers)} å€‹éœ€è¦æ‹†è§£çš„è‚¡ç¥¨")
        
        # é€£æ¥è³‡æ–™åº«
        if not self.connect_database():
            return False
        
        processed_count = 0
        failed_count = 0
        total_files_processed = 0
        
        try:
            for i, (ticker, ticker_dir) in enumerate(unprocessed_tickers, 1):
                print(f"\n{'='*60}")
                print(f"ğŸ”„ è™•ç†è‚¡ç¥¨ {i}/{len(unprocessed_tickers)}: {ticker}")
                print(f"{'='*60}")
                
                ticker_start_time = time.time()
                
                files_processed = self.process_ticker_folder(ticker)
                total_files_processed += files_processed
                
                if files_processed > 0:
                    processed_count += 1
                    ticker_duration = time.time() - ticker_start_time
                    print(f"âœ… {ticker} è™•ç†å®Œæˆï¼Œæ‹†è§£äº† {files_processed} å€‹æª”æ¡ˆï¼Œè€—æ™‚: {ticker_duration:.2f}ç§’")
                else:
                    failed_count += 1
                    print(f"âŒ {ticker} è™•ç†å¤±æ•—")
                
                # æ¯5å€‹è‚¡ç¥¨è¼¸å‡ºä¸€æ¬¡é€²åº¦
                if i % 5 == 0 or i == len(unprocessed_tickers):
                    elapsed = time.time() - self.start_time
                    progress = (i / len(unprocessed_tickers)) * 100
                    print(f"\nğŸ“Š é€²åº¦å ±å‘Š: {i}/{len(unprocessed_tickers)} ({progress:.1f}%)")
                    print(f"   âœ… æˆåŠŸ: {processed_count} | âŒ å¤±æ•—: {failed_count}")
                    print(f"   â±ï¸ å·²è€—æ™‚: {elapsed/60:.1f} åˆ†é˜")
        
        finally:
            self.disconnect_database()
        
        print(f"\n{'='*80}")
        print(f"ğŸ‰ æ‰¹é‡10-Kæ‹†è§£å®Œæˆ!")
        print(f"{'='*80}")
        print(f"ğŸ“Š æ‹†è§£çµ±è¨ˆ:")
        print(f"   âœ… æˆåŠŸæ‹†è§£è‚¡ç¥¨: {processed_count} å®¶")
        print(f"   âŒ æ‹†è§£å¤±æ•—è‚¡ç¥¨: {failed_count} å®¶")
        print(f"   ğŸ“„ ç¸½æ‹†è§£æª”æ¡ˆæ•¸: {total_files_processed} å€‹")
        
        total_time = time.time() - self.start_time
        print(f"â±ï¸ ç¸½è€—æ™‚: {total_time/60:.1f} åˆ†é˜")
        print(f"âš¡ å¹³å‡æ¯è‚¡ç¥¨: {total_time/len(unprocessed_tickers):.1f} ç§’")
        
        if processed_count > 0:
            print(f"\nğŸ¯ å»ºè­°: æ–°æ‹†è§£çš„è‚¡ç¥¨å¯ç”¨æ–¼FinBotç³»çµ±æŸ¥è©¢!")
        
        return processed_count > 0

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹æ‰¹é‡æ‹†è§£10-Kè²¡å ±ITEMå…§å®¹...")
    print("ğŸ“ æœ¬ç¨‹å¼æœƒ:")
    print("   1. æƒæ downloads è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰è‚¡ç¥¨")
    print("   2. æª¢æŸ¥æ¯å€‹è‚¡ç¥¨æ˜¯å¦å·²è¢«æ‹†è§£é")
    print("   3. å°æ–°ä¸‹è¼‰æˆ–æœªæ‹†è§£çš„è‚¡ç¥¨é€²è¡Œ ITEM æ‹†è§£")
    print("   4. å°‡çµæœå­˜å…¥ ten_k_filings è³‡æ–™è¡¨")
    print("   5. è‡ªå‹•è™•ç†æ–°ä¸‹è¼‰çš„è‚¡ç¥¨è²¡å ±")
    print("="*70)
    
    parser = BatchTenKParser()
    
    # é¦–å…ˆæª¢æŸ¥æœ‰å¤šå°‘è‚¡ç¥¨éœ€è¦æ‹†è§£
    unprocessed = parser.get_unprocessed_tickers()
    
    if not unprocessed:
        print("ğŸ‰ æ‰€æœ‰å·²ä¸‹è¼‰çš„è‚¡ç¥¨éƒ½å·²å®ŒæˆITEMæ‹†è§£!")
        print("ğŸ’¡ å¦‚éœ€æ‹†è§£æ›´å¤šè‚¡ç¥¨ï¼Œè«‹å…ˆé‹è¡Œ ALL.py ä¸‹è¼‰æ›´å¤š10-Kè²¡å ±")
        sys.exit(0)
    
    print(f"ğŸ“Š ç™¼ç¾ {len(unprocessed)} å€‹è‚¡ç¥¨éœ€è¦æ‹†è§£:")
    for i, (ticker, _) in enumerate(unprocessed[:10], 1):
        print(f"   {i:2d}. {ticker}")
    if len(unprocessed) > 10:
        print(f"   ... é‚„æœ‰ {len(unprocessed) - 10} å€‹è‚¡ç¥¨")
    
    # ç¢ºèªæ˜¯å¦ç¹¼çºŒ
    try:
        response = input(f"\nâ“ ç¢ºèªè¦æ‹†è§£é€™ {len(unprocessed)} å€‹è‚¡ç¥¨çš„10-Kè²¡å ±å—? (y/N): ")
        if response.lower() != 'y':
            print("âŒ å·²å–æ¶ˆæ‹†è§£")
            sys.exit(0)
    except KeyboardInterrupt:
        print(f"\nâŒ å·²å–æ¶ˆæ‹†è§£")
        sys.exit(0)
    
    success = parser.process_all_tickers()
    
    if success:
        print("\nâœ… æ‰¹é‡æ‹†è§£æˆåŠŸå®Œæˆ!")
        print("ğŸ¯ æ‰€æœ‰æ–°æ‹†è§£çš„è‚¡ç¥¨ç¾åœ¨å¯ä»¥åœ¨FinBotç³»çµ±ä¸­æŸ¥è©¢!")
        sys.exit(0)
    else:
        print("\nâŒ æ‰¹é‡æ‹†è§£éç¨‹ä¸­å‡ºç¾éŒ¯èª¤!")
        sys.exit(1)

if __name__ == "__main__":
    main()
