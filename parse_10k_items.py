#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
10-K Filing Items Parser and Database Inserter
è§£æ AAPL è³‡æ–™å¤¾ä¸­çš„ 10-K æª”æ¡ˆï¼Œä¸¦å°‡å„å€‹ Item å…§å®¹å­˜å…¥è³‡æ–™è¡¨
"""

import os
import re
import mysql.connector
from mysql.connector import Error
from datetime import datetime
from pathlib import Path
import hashlib

class TenKParser:
    def __init__(self, db_config=None):
        # è³‡æ–™åº«é…ç½®
        self.db_config = db_config or {
            'host': '43.207.210.147',
            'database': 'finbot_db',
            'user': 'myuser',
            'password': '123456789',
            'charset': 'utf8mb4'
        }
        
        self.db_connection = None
        
        # 10-K ä¸­å¸¸è¦‹çš„ Item é¡å‹
        self.item_patterns = {
            'item_1': r'Item\s+1\.\s+(?:Business|BUSINESS)',
            'item_1a': r'Item\s+1A\.\s+(?:Risk Factors|RISK FACTORS)',
            'item_1b': r'Item\s+1B\.\s+(?:Unresolved Staff Comments|UNRESOLVED STAFF COMMENTS)',
            'item_2': r'Item\s+2\.\s+(?:Properties|PROPERTIES)',
            'item_3': r'Item\s+3\.\s+(?:Legal Proceedings|LEGAL PROCEEDINGS)',
            'item_4': r'Item\s+4\.\s+(?:Mine Safety|MINE SAFETY)',
            'item_5': r'Item\s+5\.\s+(?:Market for Registrant|MARKET FOR REGISTRANT)',
            'item_6': r'Item\s+6\.\s+(?:Selected Financial Data|SELECTED FINANCIAL DATA)',
            'item_7': r'Item\s+7\.\s+(?:Management|MANAGEMENT)',
            'item_7a': r'Item\s+7A\.\s+(?:Quantitative and Qualitative|QUANTITATIVE AND QUALITATIVE)',
            'item_8': r'Item\s+8\.\s+(?:Financial Statements|FINANCIAL STATEMENTS)',
            'item_9': r'Item\s+9\.\s+(?:Changes in and Disagreements|CHANGES IN AND DISAGREEMENTS)',
            'item_9a': r'Item\s+9A\.\s+(?:Controls and Procedures|CONTROLS AND PROCEDURES)',
            'item_9b': r'Item\s+9B\.\s+(?:Other Information|OTHER INFORMATION)',
            'item_10': r'Item\s+10\.\s+(?:Directors|DIRECTORS)',
            'item_11': r'Item\s+11\.\s+(?:Executive Compensation|EXECUTIVE COMPENSATION)',
            'item_12': r'Item\s+12\.\s+(?:Security Ownership|SECURITY OWNERSHIP)',
            'item_13': r'Item\s+13\.\s+(?:Certain Relationships|CERTAIN RELATIONSHIPS)',
            'item_14': r'Item\s+14\.\s+(?:Principal Accountant|PRINCIPAL ACCOUNTANT)',
            'item_15': r'Item\s+15\.\s+(?:Exhibits|EXHIBITS)',
            'item_16': r'Item\s+16\.\s+(?:Form 10-K Summary|FORM 10-K SUMMARY)',
            'appendix': r'(?:INDEX\s+TO\s+FINANCIAL\s+STATEMENTS|APPENDIX|CONSOLIDATED\s+FINANCIAL\s+STATEMENTS)'
        }

    def connect_database(self):
        """é€£æ¥è³‡æ–™åº«"""
        try:
            self.db_connection = mysql.connector.connect(**self.db_config)
            if self.db_connection.is_connected():
                print("âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
                return True
        except Error as e:
            print(f"âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
            return False

    def disconnect_database(self):
        """æ–·é–‹è³‡æ–™åº«é€£æ¥"""
        if self.db_connection and self.db_connection.is_connected():
            self.db_connection.close()
            print("âœ… è³‡æ–™åº«é€£æ¥å·²é—œé–‰")

    def extract_filing_metadata(self, content):
        """æå–10-Kå ±å‘Šçš„åŸºæœ¬è³‡è¨Š"""
        metadata = {}
        
        # æå–æ–‡ä»¶ç·¨è™Ÿ
        doc_number_pattern = r'DOCUMENT\s+(\d{10}-\d{2}-\d{6})'
        doc_match = re.search(doc_number_pattern, content)
        metadata['document_number'] = doc_match.group(1) if doc_match else None
        
        # æå–å…¬å¸åç¨±
        company_pattern = r'COMPANY CONFORMED NAME:\s+(.+)'
        company_match = re.search(company_pattern, content)
        metadata['company_name'] = company_match.group(1).strip() if company_match else 'Apple Inc.'
        
        # æå–CIK
        cik_pattern = r'CENTRAL INDEX KEY:\s+(\d+)'
        cik_match = re.search(cik_pattern, content)
        metadata['cik'] = cik_match.group(1) if cik_match else None
        
        # æå–å ±å‘Šæ—¥æœŸ
        date_pattern = r'CONFORMED PERIOD OF REPORT:\s+(\d{8})'
        date_match = re.search(date_pattern, content)
        if date_match:
            date_str = date_match.group(1)
            metadata['report_date'] = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        else:
            metadata['report_date'] = None
        
        # æå–æäº¤æ—¥æœŸ
        filed_pattern = r'FILED AS OF DATE:\s+(\d{8})'
        filed_match = re.search(filed_pattern, content)
        if filed_match:
            filed_str = filed_match.group(1)
            metadata['filed_date'] = f"{filed_str[:4]}-{filed_str[4:6]}-{filed_str[6:8]}"
        else:
            metadata['filed_date'] = None
        
        return metadata

    def find_content_start_position(self, content):
        """æ‰¾åˆ°å¯¦éš›å…§å®¹é–‹å§‹ä½ç½®ï¼Œè·³éTABLE OF CONTENTS"""
        
        # ç›´æ¥å°‹æ‰¾æ‰€æœ‰ Item 1 çš„ä½ç½®ï¼Œç„¶å¾Œé¸æ“‡çœŸæ­£åŒ…å«å…§å®¹çš„é‚£ä¸€å€‹
        item1_matches = list(re.finditer(r'Item\s+1\.', content, re.IGNORECASE | re.MULTILINE))
        
        print(f"   ğŸ“ æ‰¾åˆ° {len(item1_matches)} å€‹ Item 1 ä½ç½®")
        
        for i, match in enumerate(item1_matches):
            match_pos = match.start()
            # æª¢æŸ¥é€™å€‹ä½ç½®å¾Œé¢çš„å…§å®¹
            sample_content = content[match_pos:match_pos + 1000]
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«çœŸå¯¦çš„æ¥­å‹™å…§å®¹
            if self.looks_like_real_content(sample_content):
                print(f"   ğŸ¯ é¸æ“‡ä½ç½® {i+1}: {match_pos} (åŒ…å«çœŸå¯¦å…§å®¹)")
                return match_pos
            else:
                print(f"   â­ï¸ è·³éä½ç½® {i+1}: {match_pos} (çœ‹èµ·ä¾†æ˜¯ç›®éŒ„)")
        
        # å¦‚æœéƒ½æ²’æ‰¾åˆ°çœŸå¯¦å…§å®¹ï¼Œå˜—è©¦å¾ PART I é–‹å§‹
        part_i_match = re.search(r'PART\s+I\s*[^\w]', content, re.IGNORECASE | re.MULTILINE)
        if part_i_match:
            part_i_pos = part_i_match.end()
            print(f"   ğŸ“ ä½¿ç”¨ PART I ä½ç½®: {part_i_pos}")
            return part_i_pos
        
        # æœ€å¾Œæ‰‹æ®µï¼šå¾æ–‡ä»¶çš„1/3è™•é–‹å§‹
        fallback_pos = len(content) // 3
        print(f"   ğŸ“ ä½¿ç”¨é è¨­ä½ç½®: {fallback_pos} (æª”æ¡ˆç¸½é•·åº¦: {len(content)})")
        return fallback_pos

    def looks_like_real_content(self, text):
        """æª¢æŸ¥æ–‡æœ¬æ˜¯å¦çœ‹èµ·ä¾†åƒçœŸå¯¦å…§å®¹è€Œä¸æ˜¯ç›®éŒ„æˆ–HTML"""
        # æ¸…ç† HTML æ¨™ç±¤å’Œç‰¹æ®Šå­—ç¬¦
        clean_text = re.sub(r'<[^>]+>', ' ', text)
        clean_text = re.sub(r'&#\d+;', ' ', clean_text)  # æ¸…ç†HTMLå¯¦é«”
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        # å¦‚æœå¤ªçŸ­ï¼Œå¯èƒ½ä¸æ˜¯çœŸå¯¦å…§å®¹
        if len(clean_text) < 100:
            return False
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ˜ç¢ºçš„æ¥­å‹™å…§å®¹é—œéµçŸ­èª
        business_phrases = [
            r'Company\s+Background',
            r'The\s+Company\s+designs',
            r'The\s+Company\s+manufactures',
            r'designs,\s+manufactures\s+and\s+markets',
            r'smartphones,\s+personal\s+computers',
            r'We\s+design\s+and\s+develop',
            r'Our\s+business\s+segments',
            r'fiscal\s+year.*ends',
            r'business\s+operations',
            r'Company.*designs.*manufactures'
        ]
        
        phrase_count = 0
        for phrase in business_phrases:
            if re.search(phrase, clean_text, re.IGNORECASE):
                phrase_count += 1
                
        # å¦‚æœåŒ…å«æ˜ç¢ºçš„æ¥­å‹™çŸ­èªï¼Œå°±èªç‚ºæ˜¯çœŸå¯¦å…§å®¹
        if phrase_count >= 1:
            return True
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«è¶³å¤ çš„å¯¦è³ªæ€§æ¥­å‹™é—œéµå­—
        business_keywords = [
            r'Company', r'designs', r'manufactures', r'markets', r'smartphones',
            r'personal\s+computers', r'tablets', r'wearables', r'accessories',
            r'services', r'revenue', r'fiscal\s+year', r'operations', r'products',
            r'customers', r'development', r'iPhone', r'iPad', r'Mac', r'Apple\s+Watch'
        ]
        
        keyword_count = 0
        for keyword in business_keywords:
            if re.search(keyword, clean_text, re.IGNORECASE):
                keyword_count += 1
        
        # éœ€è¦è‡³å°‘3å€‹ç›¸é—œé—œéµå­—æ‰èªç‚ºæ˜¯çœŸå¯¦å…§å®¹
        return keyword_count >= 3

    def clean_html_content(self, content):
        """å¾¹åº•æ¸…ç†HTMLæ¨™ç±¤ã€CSSæ¨£å¼å’Œç‰¹æ®Šå­—ç¬¦"""
        # ç§»é™¤ <style> æ¨™ç±¤åŠå…¶å…§å®¹
        content = re.sub(r'<style[^>]*>.*?</style>', ' ', content, flags=re.DOTALL | re.IGNORECASE)
        
        # ç§»é™¤ <script> æ¨™ç±¤åŠå…¶å…§å®¹
        content = re.sub(r'<script[^>]*>.*?</script>', ' ', content, flags=re.DOTALL | re.IGNORECASE)
        
        # ç§»é™¤æ‰€æœ‰HTMLæ¨™ç±¤
        content = re.sub(r'<[^>]+>', ' ', content)
        
        # ç§»é™¤HTMLå¯¦é«”
        content = re.sub(r'&#\d+;', ' ', content)
        content = re.sub(r'&[a-zA-Z]+;', ' ', content)
        
        # ç§»é™¤å¤šé¤˜çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content)
        
        return content.strip()

    def extract_items(self, content):
        """æå–æ‰€æœ‰Itemå…§å®¹ - æ”¹é€²ç‰ˆï¼Œå…ˆæ¸…ç†HTMLå†è·³éç›®éŒ„"""
        items = {}
        
        # ç¬¬ä¸€æ­¥ï¼šå¾¹åº•æ¸…ç†HTMLæ¨™ç±¤ã€CSSæ¨£å¼å’Œç‰¹æ®Šå­—ç¬¦
        print("   ğŸ§¹ æ¸…ç†HTMLæ¨™ç±¤å’ŒCSSæ¨£å¼...")
        original_content = content
        clean_content = self.clean_html_content(content)
        
        print(f"   ğŸ“Š æ¸…ç†å‰é•·åº¦: {len(content)}, æ¸…ç†å¾Œé•·åº¦: {len(clean_content)}")
        
        # æ‰¾åˆ°å¯¦éš›å…§å®¹é–‹å§‹ä½ç½®
        content_start = self.find_content_start_position(clean_content)
        
        # è™•ç†ä¸€èˆ¬çš„Items
        for item_key, pattern in self.item_patterns.items():
            if item_key == 'appendix':  # é™„éŒ„éœ€è¦ç‰¹æ®Šè™•ç†
                continue
                
            try:
                # å¾å…§å®¹é–‹å§‹ä½ç½®æœç´¢ç•¶å‰Item
                search_content = clean_content[content_start:]
                start_match = re.search(pattern, search_content, re.IGNORECASE | re.MULTILINE)
                
                if not start_match:
                    items[item_key] = None
                    continue
                
                # èª¿æ•´ä½ç½®ï¼ˆç›¸å°æ–¼å®Œæ•´æ¸…ç†å¾Œå…§å®¹ï¼‰
                start_pos = content_start + start_match.end()
                
                # æŸ¥æ‰¾ä¸‹ä¸€å€‹Itemçš„é–‹å§‹ä½ç½®ä½œç‚ºçµæŸé»
                next_item_pos = len(clean_content)
                for next_pattern in self.item_patterns.values():
                    if next_pattern == pattern or next_pattern == self.item_patterns['appendix']:
                        continue
                    next_match = re.search(next_pattern, clean_content[start_pos:], re.IGNORECASE | re.MULTILINE)
                    if next_match:
                        candidate_pos = start_pos + next_match.start()
                        if candidate_pos < next_item_pos:
                            next_item_pos = candidate_pos
                
                # æå–Itemå…§å®¹
                item_content = clean_content[start_pos:next_item_pos].strip()
                
                # æ¸…ç†å’Œé™åˆ¶é•·åº¦
                item_content = re.sub(r'\s+', ' ', item_content)
                if len(item_content) > 65535:  # TEXT æ¬„ä½é™åˆ¶
                    item_content = item_content[:65532] + "..."
                
                items[item_key] = item_content if item_content else None
                
                # é¡¯ç¤ºæ‰¾åˆ°çš„å…§å®¹é è¦½
                if item_content:
                    preview = item_content[:100] + "..." if len(item_content) > 100 else item_content
                    print(f"   âœ… {item_key}: {len(item_content)} å­—ç¬¦ - {preview}")
                
            except Exception as e:
                print(f"âš ï¸ æå– {item_key} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                items[item_key] = None
        
        # ç‰¹æ®Šè™•ç†é™„éŒ„
        items['appendix'] = self.extract_appendix(clean_content)
        
        return items

    def extract_appendix(self, content):
        """ç‰¹æ®Šè™•ç†é™„éŒ„å…§å®¹ - åŸºæ–¼F-é ç¢¼å’Œç›®éŒ„çµæ§‹è­˜åˆ¥ï¼ˆå„ªåŒ–ç‰ˆï¼‰"""
        try:
            # ç¬¬ä¸€æ­¥ï¼šæ‰¾åˆ°æ‰€æœ‰Itemsçš„æœ€æ™šçµæŸä½ç½®
            latest_item_end = 0
            
            # æª¢æŸ¥æ‰€æœ‰Itemsï¼Œæ‰¾åˆ°æœ€å¾Œä¸€å€‹æœ‰æ•ˆItemçš„çµæŸä½ç½®
            for item_key, pattern in self.item_patterns.items():
                if item_key == 'appendix':
                    continue
                    
                matches = list(re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE))
                if matches:
                    # å–æœ€å¾Œä¸€å€‹åŒ¹é…çš„ä½ç½®
                    latest_match = matches[-1]
                    # ä¼°ç®—è©²Itemçš„çµæŸä½ç½®ï¼ˆå‘å¾Œæœç´¢1000å€‹å­—ç¬¦ä½œç‚ºç·©è¡ï¼‰
                    item_end_estimate = latest_match.end() + 1000
                    if item_end_estimate > latest_item_end:
                        latest_item_end = item_end_estimate
            
            # å¦‚æœæ²’æ‰¾åˆ°ä»»ä½•Itemï¼Œå¾æ–‡ä»¶ä¸­é–“é–‹å§‹æœç´¢
            if latest_item_end == 0:
                latest_item_end = len(content) // 2
            
            print(f"   ğŸ“ å¾ä½ç½® {latest_item_end} é–‹å§‹æœç´¢é™„éŒ„...")
            
            # ç¬¬äºŒæ­¥ï¼šé™åˆ¶æœç´¢ç¯„åœï¼Œé¿å…æœç´¢æ•´å€‹æ–‡ä»¶
            search_content = content[latest_item_end:]
            max_search_length = min(100000, len(search_content))  # é™åˆ¶æœç´¢ç¯„åœç‚º100KB
            limited_search_content = search_content[:max_search_length]
            
            print(f"   ğŸ” æœç´¢ç¯„åœé™åˆ¶ç‚º {max_search_length} å­—ç¬¦")
            
            # ç¬¬ä¸‰æ­¥ï¼šç°¡åŒ–çš„F-é ç¢¼æœç´¢
            f_page_pattern = r'F-\d+'
            f_matches = list(re.finditer(f_page_pattern, limited_search_content))
            
            if not f_matches:
                print("   âŒ æœªæ‰¾åˆ°F-é ç¢¼ï¼Œç„¡é™„éŒ„å…§å®¹")
                return None
            
            print(f"   ğŸ“„ æ‰¾åˆ° {len(f_matches)} å€‹F-é ç¢¼")
            
            # ç¬¬å››æ­¥ï¼šå°‹æ‰¾ç¬¬ä¸€å€‹F-é ç¢¼å‰çš„ç›®éŒ„æ¨™é¡Œ
            first_f_match = f_matches[0]
            search_start = max(0, first_f_match.start() - 500)  # å‘å‰æœç´¢500å­—ç¬¦
            
            # ç°¡åŒ–çš„ç›®éŒ„æ¨¡å¼ï¼ˆé¿å…è¤‡é›œçš„æ­£å‰‡è¡¨é”å¼ï¼‰
            title_search_text = limited_search_content[search_start:first_f_match.start() + 100]
            
            # å°‹æ‰¾å¯èƒ½çš„æ¨™é¡Œé—œéµå­—
            title_keywords = [
                'INDEX TO FINANCIAL STATEMENTS',
                'FINANCIAL STATEMENTS',
                'CONSOLIDATED FINANCIAL STATEMENTS',
                'BALANCE SHEETS',
                'STATEMENTS OF OPERATIONS'
            ]
            
            appendix_start_offset = first_f_match.start()  # é è¨­å¾ç¬¬ä¸€å€‹F-é ç¢¼é–‹å§‹
            
            for keyword in title_keywords:
                keyword_pos = title_search_text.upper().find(keyword)
                if keyword_pos != -1:
                    # æ‰¾åˆ°æ¨™é¡Œï¼Œå¾æ¨™é¡Œé–‹å§‹
                    actual_pos = search_start + keyword_pos
                    if actual_pos < first_f_match.start():
                        appendix_start_offset = actual_pos
                        print(f"   ğŸ¯ æ‰¾åˆ°é™„éŒ„æ¨™é¡Œ: {keyword}")
                        break
            
            # ç¬¬äº”æ­¥ï¼šè¨ˆç®—å¯¦éš›çš„é™„éŒ„é–‹å§‹ä½ç½®
            appendix_start_pos = latest_item_end + appendix_start_offset
            
            # ç¬¬å…­æ­¥ï¼šæå–é™„éŒ„å…§å®¹ï¼ˆå¾æ‰¾åˆ°çš„ä½ç½®åˆ°æ–‡ä»¶çµå°¾ï¼‰
            appendix_content = content[appendix_start_pos:].strip()
            
            # é©—è­‰å…§å®¹ç¢ºå¯¦åŒ…å«F-é ç¢¼
            if not re.search(r'F-\d+', appendix_content[:1000]):  # åªæª¢æŸ¥å‰1000å­—ç¬¦
                print("   âš ï¸ æå–çš„å…§å®¹ä¸åŒ…å«F-é ç¢¼ï¼Œå¯èƒ½ä¸æ˜¯é™„éŒ„")
                return None
            
            # æ¸…ç†å’Œé™åˆ¶é•·åº¦
            appendix_content = re.sub(r'\s+', ' ', appendix_content)
            if len(appendix_content) > 65535:  # TEXT æ¬„ä½é™åˆ¶
                appendix_content = appendix_content[:65532] + "..."
            
            if appendix_content:
                preview = appendix_content[:100] + "..." if len(appendix_content) > 100 else appendix_content
                f_pages = re.findall(r'F-\d+', appendix_content[:1000])  # æª¢æŸ¥å‰1000å­—ç¬¦ä¸­çš„F-é ç¢¼
                print(f"   âœ… appendix: {len(appendix_content)} å­—ç¬¦ï¼ŒåŒ…å«é ç¢¼: {f_pages[:5]} - {preview}")
                return appendix_content
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ æå–é™„éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    def generate_content_hash(self, content):
        """ç”Ÿæˆå…§å®¹çš„MD5é›œæ¹Šå€¼ï¼Œç”¨æ–¼æª¢æŸ¥é‡è¤‡"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()

    def save_to_database(self, file_path, metadata, items):
        """å„²å­˜åˆ°è³‡æ–™åº«"""
        if not self.db_connection or not self.db_connection.is_connected():
            print("âŒ è³‡æ–™åº«æœªé€£æ¥")
            return False

        try:
            cursor = self.db_connection.cursor()
            
            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„æ–‡ä»¶
            content_hash = self.generate_content_hash(str(metadata) + str(items))
            check_sql = "SELECT id FROM ten_k_filings WHERE content_hash = %s"
            cursor.execute(check_sql, (content_hash,))
            
            if cursor.fetchone():
                print(f"âš ï¸ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³é: {file_path}")
                return True

            # æ’å…¥æ•¸æ“š
            insert_sql = """
                INSERT INTO ten_k_filings (
                    file_name, document_number, company_name, cik, 
                    report_date, filed_date, content_hash,
                    item_1, item_1a, item_1b, item_2, item_3, item_4,
                    item_5, item_6, item_7, item_7a, item_8, item_9,
                    item_9a, item_9b, item_10, item_11, item_12,
                    item_13, item_14, item_15, item_16, appendix,
                    created_at
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW()
                )
            """
            
            values = (
                os.path.basename(file_path),
                metadata.get('document_number'),
                metadata.get('company_name'),
                metadata.get('cik'),
                metadata.get('report_date'),
                metadata.get('filed_date'),
                content_hash,
                items.get('item_1'),
                items.get('item_1a'),
                items.get('item_1b'),
                items.get('item_2'),
                items.get('item_3'),
                items.get('item_4'),
                items.get('item_5'),
                items.get('item_6'),
                items.get('item_7'),
                items.get('item_7a'),
                items.get('item_8'),
                items.get('item_9'),
                items.get('item_9a'),
                items.get('item_9b'),
                items.get('item_10'),
                items.get('item_11'),
                items.get('item_12'),
                items.get('item_13'),
                items.get('item_14'),
                items.get('item_15'),
                items.get('item_16'),
                items.get('appendix')
            )
            
            cursor.execute(insert_sql, values)
            self.db_connection.commit()
            
            print(f"âœ… æˆåŠŸå„²å­˜: {os.path.basename(file_path)}")
            return True
            
        except Error as e:
            print(f"âŒ è³‡æ–™åº«æ’å…¥å¤±æ•—: {e}")
            return False

    def process_10k_file(self, file_path):
        """è™•ç†å–®å€‹10-Kæ–‡ä»¶"""
        print(f"\nğŸ“„ è™•ç†æ–‡ä»¶: {os.path.basename(file_path)}")
        
        try:
            # è®€å–æ–‡ä»¶å…§å®¹
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
                content = file.read()
            
            # æå–åŸºæœ¬è³‡è¨Š
            metadata = self.extract_filing_metadata(content)
            print(f"   ğŸ“Š å…¬å¸: {metadata.get('company_name', 'N/A')}")
            print(f"   ğŸ“… å ±å‘Šæ—¥æœŸ: {metadata.get('report_date', 'N/A')}")
            
            # æå–Items
            print(f"   ğŸ” æå–Items...")
            items = self.extract_items(content)
            
            # çµ±è¨ˆæœ‰æ•ˆItems
            valid_items = sum(1 for v in items.values() if v is not None)
            print(f"   âœ… æˆåŠŸæå– {valid_items}/{len(items)} å€‹Items")
            
            # å„²å­˜åˆ°è³‡æ–™åº«
            success = self.save_to_database(file_path, metadata, items)
            
            return success
            
        except Exception as e:
            print(f"âŒ è™•ç†æ–‡ä»¶å¤±æ•—: {e}")
            return False

    def process_aapl_folder(self):
        """è™•ç†AAPLè³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰10-Kæ–‡ä»¶"""
        aapl_10k_path = Path(__file__).parent / "downloads" / "AAPL" / "10-K"
        
        if not aapl_10k_path.exists():
            print(f"âŒ æ‰¾ä¸åˆ°AAPL 10-Kè³‡æ–™å¤¾: {aapl_10k_path}")
            return False
        
        # ç²å–æ‰€æœ‰.txtæ–‡ä»¶
        txt_files = list(aapl_10k_path.glob("*.txt"))
        
        if not txt_files:
            print("âŒ æ²’æœ‰æ‰¾åˆ°10-Kæ–‡ä»¶")
            return False
        
        print(f"ğŸ” æ‰¾åˆ° {len(txt_files)} å€‹10-Kæ–‡ä»¶")
        
        # é€£æ¥è³‡æ–™åº«
        if not self.connect_database():
            return False
        
        success_count = 0
        
        try:
            for file_path in txt_files:
                if self.process_10k_file(file_path):
                    success_count += 1
        
        finally:
            self.disconnect_database()
        
        print(f"\nğŸ‰ è™•ç†å®Œæˆ! æˆåŠŸ: {success_count}/{len(txt_files)}")
        return success_count > 0

def main():
    """ä¸»å‡½æ•¸"""
    parser = TenKParser()
    parser.process_aapl_folder()

if __name__ == "__main__":
    main()